{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foxnews articles current shape:\n",
      "3459\n",
      "foxnews new shape:\n",
      "3512\n",
      "\n",
      "cnn articles current shape:\n",
      "2772\n",
      "cnn new shape:\n",
      "2823\n",
      "\n",
      "msnbc articles current shape:\n",
      "533\n",
      "msnbc new shape:\n",
      "599\n",
      "\n",
      "Last run:\n",
      "2019-07-04 09:22:01.929000\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "from string import digits\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "\n",
    "#SET CRHOME DRIVER\n",
    "\n",
    "#work comp\n",
    "#driver_path = webdriver.Chrome(\"C:\\Users\\dbell\\Data Science [Github]/chromedriver.exe\")\n",
    "\n",
    "#home comp\n",
    "#driver_path = webdriver.Chrome(\"C:\\Users\\Duncan\\Data Science\\chromedriver.exe\")\n",
    "\n",
    "#collecting primary articles\n",
    "stories = []\n",
    "def getTheGoodStuff(newsstories):\n",
    "    global stories\n",
    "    global i\n",
    "    for data in newsstories:\n",
    "        htmlatag = data.find(\"h2\", class_=\"title\").find(\"a\")\n",
    "        headline = htmlatag.getText()\n",
    "        url = htmlatag.get(\"href\")\n",
    "        d = {\"headline\" : headline,\n",
    "             \"url\" : url,\n",
    "            \"sort_rank\" : i}\n",
    "        stories.append(d)\n",
    "        for related in data.find_all(class_=\"related-item\"):\n",
    "            related_htmlatag = related.find(\"a\")\n",
    "            related_headline = related_htmlatag.getText()\n",
    "            related_url = related_htmlatag.get(\"href\")\n",
    "            r = {\"headline\" : related_headline,\n",
    "                 \"url\" : related_url,\n",
    "                 \"sort_rank\" : i+1}\n",
    "            stories.append(r)\n",
    "            \n",
    "def scrapeWebsites():\n",
    "    global stories\n",
    "    global i\n",
    "    \n",
    "    # Getting stories from Fox News.\n",
    "    foxnews = \"http://www.foxnews.com/\"\n",
    "    page = urllib.urlopen(foxnews)\n",
    "    r  = requests.get(foxnews)\n",
    "    data = r.text\n",
    "    soup = BeautifulSoup(data,\"lxml\")\n",
    "    for i in range(0, 20):\n",
    "        foundstories = soup.find_all(\"article\", class_=\"article story-\" + str(i))\n",
    "        getTheGoodStuff(foundstories)\n",
    "    \n",
    "scrapeWebsites()\n",
    "articles = pd.DataFrame(stories)\n",
    "articles.drop_duplicates(inplace=True)\n",
    "articles['date'] = datetime.today().strftime('%Y-%m-%d')\n",
    "articles['year'] = datetime.today().year\n",
    "articles['month'] = datetime.today().month\n",
    "articles['day'] = datetime.today().day\n",
    "articles['hour'] = datetime.today().hour\n",
    "articles['minute'] = datetime.today().minute\n",
    "\n",
    "fox_domains = []\n",
    "for web in articles.url:\n",
    "    domain_items = web.split('//')[-1].split('/')[0].strip('www.').split('.')\n",
    "    url_item = web.split('https://www.foxnews.com/')[-1].split('/')[0]\n",
    "    if domain_items[0] in ['video','foxbusiness','nation']:\n",
    "        fox_domains.append(domain_items[0])\n",
    "    else:\n",
    "        fox_domains.append(url_item)\n",
    "articles['article_type'] = fox_domains\n",
    "\n",
    "#collecting Opinion articles\n",
    "sidebar_stories = []\n",
    "\n",
    "def opinion_getTheGoodStuff(newsstories):\n",
    "    global sidebar_stories\n",
    "    for data in newsstories:\n",
    "        htmlatag = data.find(\"h2\", class_=\"title\").find(\"a\")\n",
    "        headline = htmlatag.getText()\n",
    "        url = htmlatag.get(\"href\")\n",
    "        d = {\"headline\" : headline,\n",
    "             \"url\" : url}\n",
    "        sidebar_stories.append(d)\n",
    "\n",
    "def opinion_scrapeWebsites():\n",
    "    global sidebar_stories\n",
    "    foxnews = \"http://www.foxnews.com/\"\n",
    "    page = urllib.urlopen(foxnews)\n",
    "    r  = requests.get(foxnews)\n",
    "    data = r.text\n",
    "    soup = BeautifulSoup(data,\"lxml\")\n",
    "    foundstories = soup.find(class_=\"collection collection-opinion has-load-more js-opinion\").find_all(\"article\")\n",
    "    for article in foundstories:\n",
    "        opinion_getTheGoodStuff(foundstories)\n",
    "        \n",
    "opinion_scrapeWebsites()\n",
    "sidebar_articles = pd.DataFrame(sidebar_stories)\n",
    "sidebar_articles.drop_duplicates(inplace=True)\n",
    "sort_ranks = range(sidebar_articles.shape[0]+1)\n",
    "del sort_ranks[0]\n",
    "sidebar_articles[\"sort_rank\"] = sort_ranks\n",
    "sidebar_articles['date'] = datetime.today().strftime('%Y-%m-%d')\n",
    "sidebar_articles['year'] = datetime.today().year\n",
    "sidebar_articles['month'] = datetime.today().month\n",
    "sidebar_articles['day'] = datetime.today().day\n",
    "sidebar_articles['hour'] = datetime.today().hour\n",
    "sidebar_articles['minute'] = datetime.today().minute\n",
    "\n",
    "sidebar_articles['article_type'] = \"opinion\"\n",
    "\n",
    "all_articles = pd.concat([articles,sidebar_articles],sort=False)\n",
    "\n",
    "#harvesting article text\n",
    "article_text = []\n",
    "\n",
    "fox_driver = webdriver.Chrome(\"C:\\Users\\Duncan\\Data Science\\chromedriver.exe\")\n",
    "\n",
    "for url in all_articles.url:\n",
    "    try:\n",
    "        fox_driver.get(url)\n",
    "        sleep(1) #wait 1 second\n",
    "        html = fox_driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "        article_text.append(soup.find_all('p'))\n",
    "    except:\n",
    "        article_text.append('Error')\n",
    "\n",
    "fox_driver.close()    \n",
    "all_articles['article_text'] = article_text\n",
    "\n",
    "foxnews_articles = pd.read_csv('foxnews.csv')\n",
    "print \"foxnews articles current shape:\"\n",
    "print foxnews_articles.shape[0]\n",
    "conc = pd.concat([foxnews_articles,all_articles],sort=False)\n",
    "conc.reset_index(inplace=True)\n",
    "conc.drop(['index','Unnamed: 0'],axis=1,inplace=True)\n",
    "\n",
    "conc.to_csv('foxnews.csv', encoding = 'utf-8')\n",
    "print \"foxnews new shape:\"\n",
    "print conc.shape[0]\n",
    "print \"\"\n",
    "\n",
    "##--------------------------------------CNN scraper----------------------------------------------\n",
    "url = \"https://www.cnn.com/\"\n",
    "html = urllib.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "\n",
    "#some code to grab the relevant chunk of html containing articles\n",
    "scripts = list(soup.find_all(\"script\"))\n",
    "n=0\n",
    "for script in scripts:\n",
    "    if n==0:\n",
    "        longest_script = script\n",
    "        n=1\n",
    "    else:\n",
    "        if len(str(script)) > len(str(longest_script)):\n",
    "            longest_script = script\n",
    "        \n",
    "cnn_content = longest_script.string\n",
    "\n",
    "#distilling down to the list of dictionaries containing article content\n",
    "cnn_list = []\n",
    "for item in cnn_content.split('{\"articleList\":'):\n",
    "    for split in item.split(']}'):\n",
    "        cnn_list.append(split)\n",
    "\n",
    "cnn_articles = []        \n",
    "for stuff in cnn_list:\n",
    "    if stuff[:8] == '[{\"uri\":':\n",
    "        cnn_articles.append(stuff.encode(\"utf-8\"))\n",
    "\n",
    "articles = cnn_articles[0].replace(\"'[\",\"\") + \"]\"\n",
    "\n",
    "article_data = json.loads(articles)\n",
    "\n",
    "data = []\n",
    "\n",
    "for article in article_data:\n",
    "    article_dict = dict(article)\n",
    "    data.append([article_dict['headline'],article_dict['description'],article_dict['uri'].encode(\"utf-8\"),article_dict['duration'],article_dict['layout'],article_dict['thumbnail']])\n",
    "    \n",
    "df = pd.DataFrame(data,columns=['headline','description','url','duration','layout','thumbnail'])\n",
    "\n",
    "df['date'] = datetime.today().strftime('%Y-%m-%d')\n",
    "df['year'] = datetime.today().year\n",
    "df['month'] = datetime.today().month\n",
    "df['day'] = datetime.today().day\n",
    "df['hour'] = datetime.today().hour\n",
    "df['minute'] = datetime.today().minute\n",
    "\n",
    "sort_ranks = []\n",
    "\n",
    "for index, record in df.iterrows():\n",
    "    if record['headline'][:8] == '<strong>':\n",
    "        sort_ranks.append(1)\n",
    "    else:\n",
    "        sort_ranks.append(sort_ranks[-1]+1)\n",
    "        \n",
    "df['sort_rank'] = sort_ranks\n",
    "\n",
    "article_types = []\n",
    "\n",
    "for index, record in df.iterrows():\n",
    "    if record['duration'] <> '':\n",
    "        article_types.append(record['url'].strip('/videos/').split('/')[0])\n",
    "    else:\n",
    "        article_types.append(record['url'].translate(None, digits).strip('////').split('/')[0])\n",
    "\n",
    "df['article_type'] = article_types\n",
    "\n",
    "#harvesting article text\n",
    "article_text = []\n",
    "\n",
    "cnn_driver = webdriver.Chrome(\"C:\\Users\\Duncan\\Data Science\\chromedriver.exe\")\n",
    "\n",
    "for url in df.url:\n",
    "    try:\n",
    "        cnn_driver.get('https://www.cnn.com' + url)\n",
    "        sleep(1) #wait 1 second\n",
    "        html = cnn_driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "        article_text.append(soup.find_all(class_=re.compile(\"paragraph\")))\n",
    "    except:\n",
    "        article_text.append('Error')\n",
    "\n",
    "cnn_driver.close()    \n",
    "df['article_text'] = article_text\n",
    "\n",
    "cnn_articles = pd.read_csv('cnn.csv')\n",
    "print \"cnn articles current shape:\"\n",
    "print cnn_articles.shape[0]\n",
    "conc = pd.concat([cnn_articles,df],sort=False)\n",
    "conc.reset_index(inplace=True)\n",
    "conc.drop(['index','Unnamed: 0'],axis=1,inplace=True)\n",
    "\n",
    "conc.to_csv('cnn.csv', encoding = 'utf-8')\n",
    "print \"cnn new shape:\"\n",
    "print conc.shape[0]\n",
    "print \"\"\n",
    "\n",
    "#-----------------------------------------MSNBC Scraper----------------------------------------------------\n",
    "\n",
    "stories = []\n",
    "i = 0\n",
    "def getTheGoodStuff(newsstories):\n",
    "    global stories\n",
    "    global i\n",
    "    if newsstories.find(\"a\") <> None:\n",
    "        i=i+1\n",
    "        htmlatag = newsstories.find(\"a\")\n",
    "        url = htmlatag.get(\"href\")\n",
    "        headline = htmlatag.getText()\n",
    "        d = {\"headline\" : headline,\n",
    "             \"url\" : url,\n",
    "             \"sort_rank\" : i}\n",
    "        stories.append(d)\n",
    "            \n",
    "def scrapeMSNBC():\n",
    "    global stories\n",
    "    global newsstories\n",
    "    global i\n",
    "    # Getting stories from MSNBC\n",
    "    msnbc = \"https://www.msnbc.com/\"\n",
    "    page = urllib.urlopen(msnbc)\n",
    "    r  = requests.get(msnbc)\n",
    "    data = r.text\n",
    "    soup = BeautifulSoup(data,\"lxml\")\n",
    "    for htmlstuff in soup.find_all(class_=re.compile(\"title\")):\n",
    "        getTheGoodStuff(htmlstuff)\n",
    "        \n",
    "scrapeMSNBC()\n",
    "\n",
    "#grabbing \"related\" type stories using very similar approach\n",
    "def getrelated(newsstories):\n",
    "    global stories\n",
    "    if newsstories.find(\"a\") <> None:\n",
    "        htmlatag = newsstories.find(\"a\")\n",
    "        url = htmlatag.get(\"href\")\n",
    "        headline = htmlatag.getText()\n",
    "        d = {\"headline\" : headline,\n",
    "             \"url\" : url,\n",
    "             \"sort_rank\" : 3}\n",
    "        stories.append(d)\n",
    "            \n",
    "def scrapeMSNBC_related():\n",
    "    global stories\n",
    "    global newsstories\n",
    "    # Getting stories from MSNBC\n",
    "    msnbc = \"https://www.msnbc.com/\"\n",
    "    page = urllib.urlopen(msnbc)\n",
    "    r  = requests.get(msnbc)\n",
    "    data = r.text\n",
    "    soup = BeautifulSoup(data,\"lxml\")\n",
    "    for htmlstuff in soup.find_all(class_=re.compile(\"related\")):\n",
    "        getrelated(htmlstuff)\n",
    "                                   \n",
    "scrapeMSNBC_related()\n",
    "articles = pd.DataFrame(stories)\n",
    "articles.drop_duplicates(inplace=True)\n",
    "articles['date'] = datetime.today().strftime('%Y-%m-%d')\n",
    "articles['year'] = datetime.today().year\n",
    "articles['month'] = datetime.today().month\n",
    "articles['day'] = datetime.today().day\n",
    "articles['hour'] = datetime.today().hour\n",
    "articles['minute'] = datetime.today().minute\n",
    "\n",
    "domains = []\n",
    "for web in articles.url:\n",
    "    domain_items = web.split('//')[-1].split('/')[0].strip('www.').split('.')\n",
    "    url_item = web.split('https://www.msnbc.com/msnbc/')[-1].split('https://www.msnbc.com/')[-1].split('http://www.msnbc.com/')[-1].split('https://www.nbcnews.com/politics/')[-1].split('https://www.nbcnews.com/news/')[-1].split('https://www.nbcnews.com/tech/')[-1].split('https://www.nbcnews.com/better/')[-1].split('https://www.nbcnews.com/mach/')[-1].split('https://www.nbcnews.com/think/')[-1].split('https://www.nbcnews.com/feature/')[-1].split('https://www.nbcnews.com/')[-1].split('https://podcasts.apple.com/')[-1].split('/')[0]\n",
    "    domains.append(url_item)\n",
    "articles['article_type'] = domains\n",
    "\n",
    "#Now that we have the article URLs, now we use chrome driver to fetch text from each article\n",
    "msnbc_text = []\n",
    "\n",
    "msnbc_driver = webdriver.Chrome(\"C:\\Users\\Duncan\\Data Science\\chromedriver.exe\")\n",
    "\n",
    "for url in articles.url:\n",
    "    try:\n",
    "        msnbc_driver.get(url)\n",
    "        sleep(1) #wait 1 second\n",
    "        html = msnbc_driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "        msnbc_text.append(soup.find_all('p'))\n",
    "    except:\n",
    "        msnbc_text.append('Error')\n",
    "\n",
    "msnbc_driver.close()    \n",
    "articles['article_text'] = msnbc_text\n",
    "\n",
    "msnbc_articles = pd.read_csv('msnbc.csv')\n",
    "print \"msnbc articles current shape:\"\n",
    "print msnbc_articles.shape[0]\n",
    "conc = pd.concat([msnbc_articles,articles],sort=False)\n",
    "conc.reset_index(inplace=True)\n",
    "conc.drop(['index','Unnamed: 0'],axis=1,inplace=True)\n",
    "\n",
    "conc.to_csv('msnbc.csv', encoding = 'utf-8')\n",
    "print \"msnbc new shape:\"\n",
    "print conc.shape[0]\n",
    "print \"\"\n",
    "\n",
    "print \"Last run:\"\n",
    "print datetime.today()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
