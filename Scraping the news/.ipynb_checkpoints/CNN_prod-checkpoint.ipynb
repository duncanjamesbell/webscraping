{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Duncan\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:177: UserWarning: You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\n",
      "  warnings.warn(\"You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn articles current shape:\n",
      "56\n",
      "\n",
      "cnn new shape:\n",
      "103\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "from string import digits\n",
    "import re\n",
    "\n",
    "url = \"https://www.cnn.com/\"\n",
    "html = urllib.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "\n",
    "#some code to grab the relevant chunk of html containing articles\n",
    "scripts = list(soup.find_all(\"script\"))\n",
    "n=0\n",
    "for script in scripts:\n",
    "    if n==0:\n",
    "        longest_script = script\n",
    "        n=1\n",
    "    else:\n",
    "        if len(str(script)) > len(str(longest_script)):\n",
    "            longest_script = script\n",
    "        \n",
    "cnn_content = longest_script.string\n",
    "\n",
    "#distilling down to the list of dictionaries containing article content\n",
    "cnn_list = []\n",
    "for item in cnn_content.split('{\"articleList\":'):\n",
    "    for split in item.split(']}'):\n",
    "        cnn_list.append(split)\n",
    "\n",
    "cnn_articles = []        \n",
    "for stuff in cnn_list:\n",
    "    if stuff[:8] == '[{\"uri\":':\n",
    "        cnn_articles.append(stuff.encode(\"utf-8\"))\n",
    "\n",
    "articles = cnn_articles[0].replace(\"'[\",\"\") + \"]\"\n",
    "\n",
    "article_data = json.loads(articles)\n",
    "\n",
    "data = []\n",
    "\n",
    "for article in article_data:\n",
    "    article_dict = dict(article)\n",
    "    data.append([article_dict['headline'],article_dict['description'],article_dict['uri'].encode(\"utf-8\"),article_dict['duration'],article_dict['layout'],article_dict['thumbnail']])\n",
    "    \n",
    "df = pd.DataFrame(data,columns=['headline','description','url','duration','layout','thumbnail'])\n",
    "\n",
    "df['date'] = datetime.today().strftime('%Y-%m-%d')\n",
    "df['year'] = datetime.today().year\n",
    "df['month'] = datetime.today().month\n",
    "df['day'] = datetime.today().day\n",
    "df['hour'] = datetime.today().hour\n",
    "df['minute'] = datetime.today().minute\n",
    "\n",
    "sort_ranks = []\n",
    "\n",
    "for index, record in df.iterrows():\n",
    "    if record['headline'][:8] == '<strong>':\n",
    "        sort_ranks.append(1)\n",
    "    else:\n",
    "        sort_ranks.append(sort_ranks[-1]+1)\n",
    "        \n",
    "df['sort_rank'] = sort_ranks\n",
    "\n",
    "article_types = []\n",
    "\n",
    "for index, record in df.iterrows():\n",
    "    if record['duration'] <> '':\n",
    "        article_types.append(record['url'].strip('/videos/').split('/')[0])\n",
    "    else:\n",
    "        article_types.append(record['url'].translate(None, digits).strip('////').split('/')[0])\n",
    "\n",
    "df['article_type'] = article_types\n",
    "\n",
    "#harvesting article text\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "\n",
    "article_text = []\n",
    "#work comp\n",
    "#driver = webdriver.Chrome(\"C:\\Users\\dbell\\Data Science [Github]/chromedriver.exe\")\n",
    "\n",
    "#home comp\n",
    "driver = webdriver.Chrome(\"C:\\Users\\Duncan\\Data Science\\chromedriver.exe\")\n",
    "\n",
    "for url in df.url:\n",
    "    driver.get('https://www.cnn.com' + url)\n",
    "    sleep(1) #wait 1 second\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "    article_text.append(soup.find_all(class_=re.compile(\"paragraph\")))\n",
    "\n",
    "driver.close()    \n",
    "df['article_text'] = article_text\n",
    "\n",
    "cnn_articles = pd.read_csv('cnn.csv')\n",
    "print \"cnn articles current shape:\"\n",
    "print cnn_articles.shape[0]\n",
    "print \"\"\n",
    "conc = pd.concat([cnn_articles,df],sort=False)\n",
    "conc.reset_index(inplace=True)\n",
    "conc.drop(['index','Unnamed: 0'],axis=1,inplace=True)\n",
    "\n",
    "conc.to_csv('cnn.csv', encoding = 'utf-8')\n",
    "print \"cnn new shape:\"\n",
    "print conc.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
