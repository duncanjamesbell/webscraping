{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foxnews articles current shape:\n",
      "3362\n",
      "foxnews new shape:\n",
      "3408\n",
      "\n"
     ]
    },
    {
     "ename": "TimeoutException",
     "evalue": "Message: timeout\n  (Session info: chrome=75.0.3770.100)\n  (Driver info: chromedriver=74.0.3729.6 (255758eccf3d244491b8a1317aa76e1ce10d57e9-refs/branch-heads/3729@{#29}),platform=Windows NT 10.0.17134 x86_64)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mTimeoutException\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-5a597569d3ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m     \u001b[0mcnn_driver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://www.cnn.com'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m     \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#wait 1 second\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn_driver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Duncan\\Anaconda2\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[0mLoads\u001b[0m \u001b[0ma\u001b[0m \u001b[0mweb\u001b[0m \u001b[0mpage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mbrowser\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \"\"\"\n\u001b[1;32m--> 333\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGET\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'url'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Duncan\\Anaconda2\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.pyc\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32mC:\\Users\\Duncan\\Anaconda2\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.pyc\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTimeoutException\u001b[0m: Message: timeout\n  (Session info: chrome=75.0.3770.100)\n  (Driver info: chromedriver=74.0.3729.6 (255758eccf3d244491b8a1317aa76e1ce10d57e9-refs/branch-heads/3729@{#29}),platform=Windows NT 10.0.17134 x86_64)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "from string import digits\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "\n",
    "#SET CRHOME DRIVER\n",
    "\n",
    "#work comp\n",
    "#driver_path = webdriver.Chrome(\"C:\\Users\\dbell\\Data Science [Github]/chromedriver.exe\")\n",
    "\n",
    "#home comp\n",
    "#driver_path = webdriver.Chrome(\"C:\\Users\\Duncan\\Data Science\\chromedriver.exe\")\n",
    "\n",
    "#collecting primary articles\n",
    "stories = []\n",
    "def getTheGoodStuff(newsstories):\n",
    "    global stories\n",
    "    global i\n",
    "    for data in newsstories:\n",
    "        htmlatag = data.find(\"h2\", class_=\"title\").find(\"a\")\n",
    "        headline = htmlatag.getText()\n",
    "        url = htmlatag.get(\"href\")\n",
    "        d = {\"headline\" : headline,\n",
    "             \"url\" : url,\n",
    "            \"sort_rank\" : i}\n",
    "        stories.append(d)\n",
    "        for related in data.find_all(class_=\"related-item\"):\n",
    "            related_htmlatag = related.find(\"a\")\n",
    "            related_headline = related_htmlatag.getText()\n",
    "            related_url = related_htmlatag.get(\"href\")\n",
    "            r = {\"headline\" : related_headline,\n",
    "                 \"url\" : related_url,\n",
    "                 \"sort_rank\" : i+1}\n",
    "            stories.append(r)\n",
    "            \n",
    "def scrapeWebsites():\n",
    "    global stories\n",
    "    global i\n",
    "    \n",
    "    # Getting stories from Fox News.\n",
    "    foxnews = \"http://www.foxnews.com/\"\n",
    "    page = urllib.urlopen(foxnews)\n",
    "    r  = requests.get(foxnews)\n",
    "    data = r.text\n",
    "    soup = BeautifulSoup(data,\"lxml\")\n",
    "    for i in range(0, 20):\n",
    "        foundstories = soup.find_all(\"article\", class_=\"article story-\" + str(i))\n",
    "        getTheGoodStuff(foundstories)\n",
    "    \n",
    "scrapeWebsites()\n",
    "articles = pd.DataFrame(stories)\n",
    "articles.drop_duplicates(inplace=True)\n",
    "articles['date'] = datetime.today().strftime('%Y-%m-%d')\n",
    "articles['year'] = datetime.today().year\n",
    "articles['month'] = datetime.today().month\n",
    "articles['day'] = datetime.today().day\n",
    "articles['hour'] = datetime.today().hour\n",
    "articles['minute'] = datetime.today().minute\n",
    "\n",
    "fox_domains = []\n",
    "for web in articles.url:\n",
    "    domain_items = web.split('//')[-1].split('/')[0].strip('www.').split('.')\n",
    "    url_item = web.split('https://www.foxnews.com/')[-1].split('/')[0]\n",
    "    if domain_items[0] in ['video','foxbusiness','nation']:\n",
    "        fox_domains.append(domain_items[0])\n",
    "    else:\n",
    "        fox_domains.append(url_item)\n",
    "articles['article_type'] = fox_domains\n",
    "\n",
    "#collecting Opinion articles\n",
    "sidebar_stories = []\n",
    "\n",
    "def opinion_getTheGoodStuff(newsstories):\n",
    "    global sidebar_stories\n",
    "    for data in newsstories:\n",
    "        htmlatag = data.find(\"h2\", class_=\"title\").find(\"a\")\n",
    "        headline = htmlatag.getText()\n",
    "        url = htmlatag.get(\"href\")\n",
    "        d = {\"headline\" : headline,\n",
    "             \"url\" : url}\n",
    "        sidebar_stories.append(d)\n",
    "\n",
    "def opinion_scrapeWebsites():\n",
    "    global sidebar_stories\n",
    "    foxnews = \"http://www.foxnews.com/\"\n",
    "    page = urllib.urlopen(foxnews)\n",
    "    r  = requests.get(foxnews)\n",
    "    data = r.text\n",
    "    soup = BeautifulSoup(data,\"lxml\")\n",
    "    foundstories = soup.find(class_=\"collection collection-opinion has-load-more js-opinion\").find_all(\"article\")\n",
    "    for article in foundstories:\n",
    "        opinion_getTheGoodStuff(foundstories)\n",
    "        \n",
    "opinion_scrapeWebsites()\n",
    "sidebar_articles = pd.DataFrame(sidebar_stories)\n",
    "sidebar_articles.drop_duplicates(inplace=True)\n",
    "sort_ranks = range(sidebar_articles.shape[0]+1)\n",
    "del sort_ranks[0]\n",
    "sidebar_articles[\"sort_rank\"] = sort_ranks\n",
    "sidebar_articles['date'] = datetime.today().strftime('%Y-%m-%d')\n",
    "sidebar_articles['year'] = datetime.today().year\n",
    "sidebar_articles['month'] = datetime.today().month\n",
    "sidebar_articles['day'] = datetime.today().day\n",
    "sidebar_articles['hour'] = datetime.today().hour\n",
    "sidebar_articles['minute'] = datetime.today().minute\n",
    "\n",
    "sidebar_articles['article_type'] = \"opinion\"\n",
    "\n",
    "all_articles = pd.concat([articles,sidebar_articles],sort=False)\n",
    "\n",
    "#harvesting article text\n",
    "article_text = []\n",
    "\n",
    "fox_driver = webdriver.Chrome(\"C:\\Users\\Duncan\\Data Science\\chromedriver.exe\")\n",
    "\n",
    "for url in all_articles.url:\n",
    "    try:\n",
    "        fox_driver.get(url)\n",
    "        sleep(1) #wait 1 second\n",
    "        html = fox_driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "        article_text.append(soup.find_all('p'))\n",
    "    except:\n",
    "        article_text.append('Error')\n",
    "\n",
    "fox_driver.close()    \n",
    "all_articles['article_text'] = article_text\n",
    "\n",
    "foxnews_articles = pd.read_csv('foxnews.csv')\n",
    "print \"foxnews articles current shape:\"\n",
    "print foxnews_articles.shape[0]\n",
    "conc = pd.concat([foxnews_articles,all_articles],sort=False)\n",
    "conc.reset_index(inplace=True)\n",
    "conc.drop(['index','Unnamed: 0'],axis=1,inplace=True)\n",
    "\n",
    "conc.to_csv('foxnews.csv', encoding = 'utf-8')\n",
    "print \"foxnews new shape:\"\n",
    "print conc.shape[0]\n",
    "print \"\"\n",
    "\n",
    "##--------------------------------------CNN scraper----------------------------------------------\n",
    "url = \"https://www.cnn.com/\"\n",
    "html = urllib.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "\n",
    "#some code to grab the relevant chunk of html containing articles\n",
    "scripts = list(soup.find_all(\"script\"))\n",
    "n=0\n",
    "for script in scripts:\n",
    "    if n==0:\n",
    "        longest_script = script\n",
    "        n=1\n",
    "    else:\n",
    "        if len(str(script)) > len(str(longest_script)):\n",
    "            longest_script = script\n",
    "        \n",
    "cnn_content = longest_script.string\n",
    "\n",
    "#distilling down to the list of dictionaries containing article content\n",
    "cnn_list = []\n",
    "for item in cnn_content.split('{\"articleList\":'):\n",
    "    for split in item.split(']}'):\n",
    "        cnn_list.append(split)\n",
    "\n",
    "cnn_articles = []        \n",
    "for stuff in cnn_list:\n",
    "    if stuff[:8] == '[{\"uri\":':\n",
    "        cnn_articles.append(stuff.encode(\"utf-8\"))\n",
    "\n",
    "articles = cnn_articles[0].replace(\"'[\",\"\") + \"]\"\n",
    "\n",
    "article_data = json.loads(articles)\n",
    "\n",
    "data = []\n",
    "\n",
    "for article in article_data:\n",
    "    article_dict = dict(article)\n",
    "    data.append([article_dict['headline'],article_dict['description'],article_dict['uri'].encode(\"utf-8\"),article_dict['duration'],article_dict['layout'],article_dict['thumbnail']])\n",
    "    \n",
    "df = pd.DataFrame(data,columns=['headline','description','url','duration','layout','thumbnail'])\n",
    "\n",
    "df['date'] = datetime.today().strftime('%Y-%m-%d')\n",
    "df['year'] = datetime.today().year\n",
    "df['month'] = datetime.today().month\n",
    "df['day'] = datetime.today().day\n",
    "df['hour'] = datetime.today().hour\n",
    "df['minute'] = datetime.today().minute\n",
    "\n",
    "sort_ranks = []\n",
    "\n",
    "for index, record in df.iterrows():\n",
    "    if record['headline'][:8] == '<strong>':\n",
    "        sort_ranks.append(1)\n",
    "    else:\n",
    "        sort_ranks.append(sort_ranks[-1]+1)\n",
    "        \n",
    "df['sort_rank'] = sort_ranks\n",
    "\n",
    "article_types = []\n",
    "\n",
    "for index, record in df.iterrows():\n",
    "    if record['duration'] <> '':\n",
    "        article_types.append(record['url'].strip('/videos/').split('/')[0])\n",
    "    else:\n",
    "        article_types.append(record['url'].translate(None, digits).strip('////').split('/')[0])\n",
    "\n",
    "df['article_type'] = article_types\n",
    "\n",
    "#harvesting article text\n",
    "article_text = []\n",
    "\n",
    "cnn_driver = webdriver.Chrome(\"C:\\Users\\Duncan\\Data Science\\chromedriver.exe\")\n",
    "\n",
    "for url in df.url:\n",
    "    try:\n",
    "        cnn_driver.get('https://www.cnn.com' + url)\n",
    "        sleep(1) #wait 1 second\n",
    "        html = cnn_driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "        article_text.append(soup.find_all(class_=re.compile(\"paragraph\")))\n",
    "    except:\n",
    "        article_text.append('Error')\n",
    "\n",
    "cnn_driver.close()    \n",
    "df['article_text'] = article_text\n",
    "\n",
    "cnn_articles = pd.read_csv('cnn.csv')\n",
    "print \"cnn articles current shape:\"\n",
    "print cnn_articles.shape[0]\n",
    "conc = pd.concat([cnn_articles,df],sort=False)\n",
    "conc.reset_index(inplace=True)\n",
    "conc.drop(['index','Unnamed: 0'],axis=1,inplace=True)\n",
    "\n",
    "conc.to_csv('cnn.csv', encoding = 'utf-8')\n",
    "print \"cnn new shape:\"\n",
    "print conc.shape[0]\n",
    "print \"\"\n",
    "\n",
    "#-----------------------------------------MSNBC Scraper----------------------------------------------------\n",
    "\n",
    "stories = []\n",
    "i = 0\n",
    "def getTheGoodStuff(newsstories):\n",
    "    global stories\n",
    "    global i\n",
    "    if newsstories.find(\"a\") <> None:\n",
    "        i=i+1\n",
    "        htmlatag = newsstories.find(\"a\")\n",
    "        url = htmlatag.get(\"href\")\n",
    "        headline = htmlatag.getText()\n",
    "        d = {\"headline\" : headline,\n",
    "             \"url\" : url,\n",
    "             \"sort_rank\" : i}\n",
    "        stories.append(d)\n",
    "            \n",
    "def scrapeMSNBC():\n",
    "    global stories\n",
    "    global newsstories\n",
    "    global i\n",
    "    # Getting stories from MSNBC\n",
    "    msnbc = \"https://www.msnbc.com/\"\n",
    "    page = urllib.urlopen(msnbc)\n",
    "    r  = requests.get(msnbc)\n",
    "    data = r.text\n",
    "    soup = BeautifulSoup(data,\"lxml\")\n",
    "    for htmlstuff in soup.find_all(class_=re.compile(\"title\")):\n",
    "        getTheGoodStuff(htmlstuff)\n",
    "        \n",
    "scrapeMSNBC()\n",
    "\n",
    "#grabbing \"related\" type stories using very similar approach\n",
    "def getrelated(newsstories):\n",
    "    global stories\n",
    "    if newsstories.find(\"a\") <> None:\n",
    "        htmlatag = newsstories.find(\"a\")\n",
    "        url = htmlatag.get(\"href\")\n",
    "        headline = htmlatag.getText()\n",
    "        d = {\"headline\" : headline,\n",
    "             \"url\" : url,\n",
    "             \"sort_rank\" : 3}\n",
    "        stories.append(d)\n",
    "            \n",
    "def scrapeMSNBC_related():\n",
    "    global stories\n",
    "    global newsstories\n",
    "    # Getting stories from MSNBC\n",
    "    msnbc = \"https://www.msnbc.com/\"\n",
    "    page = urllib.urlopen(msnbc)\n",
    "    r  = requests.get(msnbc)\n",
    "    data = r.text\n",
    "    soup = BeautifulSoup(data,\"lxml\")\n",
    "    for htmlstuff in soup.find_all(class_=re.compile(\"related\")):\n",
    "        getrelated(htmlstuff)\n",
    "                                   \n",
    "scrapeMSNBC_related()\n",
    "articles = pd.DataFrame(stories)\n",
    "articles.drop_duplicates(inplace=True)\n",
    "articles['date'] = datetime.today().strftime('%Y-%m-%d')\n",
    "articles['year'] = datetime.today().year\n",
    "articles['month'] = datetime.today().month\n",
    "articles['day'] = datetime.today().day\n",
    "articles['hour'] = datetime.today().hour\n",
    "articles['minute'] = datetime.today().minute\n",
    "\n",
    "domains = []\n",
    "for web in articles.url:\n",
    "    domain_items = web.split('//')[-1].split('/')[0].strip('www.').split('.')\n",
    "    url_item = web.split('https://www.msnbc.com/msnbc/')[-1].split('https://www.msnbc.com/')[-1].split('http://www.msnbc.com/')[-1].split('https://www.nbcnews.com/politics/')[-1].split('https://www.nbcnews.com/news/')[-1].split('https://www.nbcnews.com/tech/')[-1].split('https://www.nbcnews.com/better/')[-1].split('https://www.nbcnews.com/mach/')[-1].split('https://www.nbcnews.com/think/')[-1].split('https://www.nbcnews.com/feature/')[-1].split('https://www.nbcnews.com/')[-1].split('https://podcasts.apple.com/')[-1].split('/')[0]\n",
    "    domains.append(url_item)\n",
    "articles['domain'] = domains\n",
    "\n",
    "#Now that we have the article URLs, now we use chrome driver to fetch text from each article\n",
    "msnbc_text = []\n",
    "\n",
    "msnbc_driver = webdriver.Chrome(\"C:\\Users\\Duncan\\Data Science\\chromedriver.exe\")\n",
    "\n",
    "for url in articles.url:\n",
    "    try:\n",
    "        msnbc_driver.get(url)\n",
    "        sleep(1) #wait 1 second\n",
    "        html = msnbc_driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "        msnbc_text.append(soup.find_all('p'))\n",
    "    except:\n",
    "        msnbc_text.append('Error')\n",
    "\n",
    "msnbc_driver.close()    \n",
    "articles['article_text'] = msnbc_text\n",
    "\n",
    "msnbc_articles = pd.read_csv('msnbc.csv')\n",
    "print \"msnbc articles current shape:\"\n",
    "print msnbc_articles.shape[0]\n",
    "conc = pd.concat([msnbc_articles,articles],sort=False)\n",
    "conc.reset_index(inplace=True)\n",
    "conc.drop(['index','Unnamed: 0'],axis=1,inplace=True)\n",
    "\n",
    "conc.to_csv('msnbc.csv', encoding = 'utf-8')\n",
    "print \"msnbc new shape:\"\n",
    "print conc.shape[0]\n",
    "print \"\"\n",
    "\n",
    "print \"Last run:\"\n",
    "print datetime.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn articles current shape:\n",
      "2723\n",
      "\n",
      "cnn new shape:\n",
      "2777\n",
      "\n",
      "Last run:\n",
      "2019-07-03 08:35:43.907000\n"
     ]
    }
   ],
   "source": [
    "#run CNN only\n",
    "url = \"https://www.cnn.com/\"\n",
    "html = urllib.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "\n",
    "#some code to grab the relevant chunk of html containing articles\n",
    "scripts = list(soup.find_all(\"script\"))\n",
    "n=0\n",
    "for script in scripts:\n",
    "    if n==0:\n",
    "        longest_script = script\n",
    "        n=1\n",
    "    else:\n",
    "        if len(str(script)) > len(str(longest_script)):\n",
    "            longest_script = script\n",
    "        \n",
    "cnn_content = longest_script.string\n",
    "\n",
    "#distilling down to the list of dictionaries containing article content\n",
    "cnn_list = []\n",
    "for item in cnn_content.split('{\"articleList\":'):\n",
    "    for split in item.split(']}'):\n",
    "        cnn_list.append(split)\n",
    "\n",
    "cnn_articles = []        \n",
    "for stuff in cnn_list:\n",
    "    if stuff[:8] == '[{\"uri\":':\n",
    "        cnn_articles.append(stuff.encode(\"utf-8\"))\n",
    "\n",
    "articles = cnn_articles[0].replace(\"'[\",\"\") + \"]\"\n",
    "\n",
    "article_data = json.loads(articles)\n",
    "\n",
    "data = []\n",
    "\n",
    "for article in article_data:\n",
    "    article_dict = dict(article)\n",
    "    data.append([article_dict['headline'],article_dict['description'],article_dict['uri'].encode(\"utf-8\"),article_dict['duration'],article_dict['layout'],article_dict['thumbnail']])\n",
    "    \n",
    "df = pd.DataFrame(data,columns=['headline','description','url','duration','layout','thumbnail'])\n",
    "\n",
    "df['date'] = datetime.today().strftime('%Y-%m-%d')\n",
    "df['year'] = datetime.today().year\n",
    "df['month'] = datetime.today().month\n",
    "df['day'] = datetime.today().day\n",
    "df['hour'] = datetime.today().hour\n",
    "df['minute'] = datetime.today().minute\n",
    "\n",
    "sort_ranks = []\n",
    "\n",
    "for index, record in df.iterrows():\n",
    "    if record['headline'][:8] == '<strong>':\n",
    "        sort_ranks.append(1)\n",
    "    else:\n",
    "        sort_ranks.append(sort_ranks[-1]+1)\n",
    "        \n",
    "df['sort_rank'] = sort_ranks\n",
    "\n",
    "article_types = []\n",
    "\n",
    "for index, record in df.iterrows():\n",
    "    if record['duration'] <> '':\n",
    "        article_types.append(record['url'].strip('/videos/').split('/')[0])\n",
    "    else:\n",
    "        article_types.append(record['url'].translate(None, digits).strip('////').split('/')[0])\n",
    "\n",
    "df['article_type'] = article_types\n",
    "\n",
    "#harvesting article text\n",
    "article_text = []\n",
    "\n",
    "cnn_driver = webdriver.Chrome(\"C:\\Users\\Duncan\\Data Science\\chromedriver.exe\")\n",
    "\n",
    "for url in df.url:\n",
    "    try:\n",
    "        cnn_driver.get('https://www.cnn.com' + url)\n",
    "        sleep(1) #wait 1 second\n",
    "        html = cnn_driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "        article_text.append(soup.find_all(class_=re.compile(\"paragraph\")))\n",
    "    except:\n",
    "        article_text.append('Error')\n",
    "\n",
    "cnn_driver.close()    \n",
    "df['article_text'] = article_text\n",
    "\n",
    "cnn_articles = pd.read_csv('cnn.csv')\n",
    "print \"cnn articles current shape:\"\n",
    "print cnn_articles.shape[0]\n",
    "print \"\"\n",
    "conc = pd.concat([cnn_articles,df],sort=False)\n",
    "conc.reset_index(inplace=True)\n",
    "conc.drop(['index','Unnamed: 0'],axis=1,inplace=True)\n",
    "\n",
    "conc.to_csv('cnn2.csv', encoding = 'utf-8')\n",
    "print \"cnn new shape:\"\n",
    "print conc.shape[0]\n",
    "print \"\"\n",
    "\n",
    "print \"Last run:\"\n",
    "print datetime.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msnbc articles current shape:\n",
      "399\n",
      "\n",
      "msnbc new shape:\n",
      "467\n",
      "\n",
      "Last run:\n",
      "2019-07-03 11:16:07.858000\n"
     ]
    }
   ],
   "source": [
    "#run MSNBC only\n",
    "\n",
    "stories = []\n",
    "i = 0\n",
    "def getTheGoodStuff(newsstories):\n",
    "    global stories\n",
    "    global i\n",
    "    if newsstories.find(\"a\") <> None:\n",
    "        i=i+1\n",
    "        htmlatag = newsstories.find(\"a\")\n",
    "        url = htmlatag.get(\"href\")\n",
    "        headline = htmlatag.getText()\n",
    "        d = {\"headline\" : headline,\n",
    "             \"url\" : url,\n",
    "             \"sort_rank\" : i}\n",
    "        stories.append(d)\n",
    "            \n",
    "def scrapeMSNBC():\n",
    "    global stories\n",
    "    global newsstories\n",
    "    global i\n",
    "    # Getting stories from MSNBC\n",
    "    msnbc = \"https://www.msnbc.com/\"\n",
    "    page = urllib.urlopen(msnbc)\n",
    "    r  = requests.get(msnbc)\n",
    "    data = r.text\n",
    "    soup = BeautifulSoup(data,\"lxml\")\n",
    "    for htmlstuff in soup.find_all(class_=re.compile(\"title\")):\n",
    "        getTheGoodStuff(htmlstuff)\n",
    "        \n",
    "scrapeMSNBC()\n",
    "\n",
    "#grabbing \"related\" type stories using very similar approach\n",
    "def getrelated(newsstories):\n",
    "    global stories\n",
    "    if newsstories.find(\"a\") <> None:\n",
    "        htmlatag = newsstories.find(\"a\")\n",
    "        url = htmlatag.get(\"href\")\n",
    "        headline = htmlatag.getText()\n",
    "        d = {\"headline\" : headline,\n",
    "             \"url\" : url,\n",
    "             \"sort_rank\" : 3}\n",
    "        stories.append(d)\n",
    "            \n",
    "def scrapeMSNBC_related():\n",
    "    global stories\n",
    "    global newsstories\n",
    "    # Getting stories from MSNBC\n",
    "    msnbc = \"https://www.msnbc.com/\"\n",
    "    page = urllib.urlopen(msnbc)\n",
    "    r  = requests.get(msnbc)\n",
    "    data = r.text\n",
    "    soup = BeautifulSoup(data,\"lxml\")\n",
    "    for htmlstuff in soup.find_all(class_=re.compile(\"related\")):\n",
    "        getrelated(htmlstuff)\n",
    "                                   \n",
    "scrapeMSNBC_related()\n",
    "articles = pd.DataFrame(stories)\n",
    "articles.drop_duplicates(inplace=True)\n",
    "articles['date'] = datetime.today().strftime('%Y-%m-%d')\n",
    "articles['year'] = datetime.today().year\n",
    "articles['month'] = datetime.today().month\n",
    "articles['day'] = datetime.today().day\n",
    "articles['hour'] = datetime.today().hour\n",
    "articles['minute'] = datetime.today().minute\n",
    "\n",
    "domains = []\n",
    "for web in articles.url:\n",
    "    domain_items = web.split('//')[-1].split('/')[0].strip('www.').split('.')\n",
    "    url_item = web.split('https://www.msnbc.com/msnbc/')[-1].split('https://www.msnbc.com/')[-1].split('http://www.msnbc.com/')[-1].split('https://www.nbcnews.com/politics/')[-1].split('https://www.nbcnews.com/news/')[-1].split('https://www.nbcnews.com/tech/')[-1].split('https://www.nbcnews.com/better/')[-1].split('https://www.nbcnews.com/mach/')[-1].split('https://www.nbcnews.com/think/')[-1].split('https://www.nbcnews.com/feature/')[-1].split('https://www.nbcnews.com/')[-1].split('https://podcasts.apple.com/')[-1].split('/')[0]\n",
    "    domains.append(url_item)\n",
    "articles['domain'] = domains\n",
    "\n",
    "#Now that we have the article URLs, now we use chrome driver to fetch text from each article\n",
    "msnbc_text = []\n",
    "\n",
    "msnbc_driver = webdriver.Chrome(\"C:\\Users\\Duncan\\Data Science\\chromedriver.exe\")\n",
    "\n",
    "for url in articles.url:\n",
    "    msnbc_driver.get(url)\n",
    "    sleep(1) #wait 1 second\n",
    "    html = msnbc_driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "    msnbc_text.append(soup.find_all('p'))\n",
    "\n",
    "msnbc_driver.close()    \n",
    "articles['article_text'] = msnbc_text\n",
    "\n",
    "msnbc_articles = pd.read_csv('msnbc.csv')\n",
    "print \"msnbc articles current shape:\"\n",
    "print msnbc_articles.shape[0]\n",
    "print \"\"\n",
    "conc = pd.concat([msnbc_articles,articles],sort=False)\n",
    "conc.reset_index(inplace=True)\n",
    "conc.drop(['index','Unnamed: 0'],axis=1,inplace=True)\n",
    "\n",
    "conc.to_csv('msnbc.csv', encoding = 'utf-8')\n",
    "print \"msnbc new shape:\"\n",
    "print conc.shape[0]\n",
    "print \"\"\n",
    "\n",
    "print \"Last run:\"\n",
    "print datetime.today()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
