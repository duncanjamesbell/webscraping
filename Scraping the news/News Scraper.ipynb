{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foxnews articles current shape:\n",
      "3114\n",
      "\n",
      "foxnews new shape:\n",
      "3164\n",
      "cnn articles current shape:\n",
      "2394\n",
      "\n",
      "cnn new shape:\n",
      "2453\n",
      "msnbc articles current shape:\n",
      "67\n",
      "\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 13] Permission denied: 'msnbc.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mIOError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-7de0ac18d7de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[0mconc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Unnamed: 0'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m \u001b[0mconc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'msnbc.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"msnbc new shape:\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mconc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Duncan\\Anaconda2\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   3018\u001b[0m                                  \u001b[0mdoublequote\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3019\u001b[0m                                  escapechar=escapechar, decimal=decimal)\n\u001b[1;32m-> 3020\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3021\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3022\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Duncan\\Anaconda2\\lib\\site-packages\\pandas\\io\\formats\\csvs.pyc\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    155\u001b[0m             f, handles = _get_handle(self.path_or_buf, self.mode,\n\u001b[0;32m    156\u001b[0m                                      \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m                                      compression=self.compression)\n\u001b[0m\u001b[0;32m    158\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Duncan\\Anaconda2\\lib\\site-packages\\pandas\\io\\common.pyc\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    419\u001b[0m             \u001b[1;31m# Python 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m             \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"wb\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"w\"\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m             \u001b[1;31m# Python 3 and encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 13] Permission denied: 'msnbc.csv'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "from string import digits\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "\n",
    "#SET CRHOME DRIVER\n",
    "\n",
    "#work comp\n",
    "#driver_path = webdriver.Chrome(\"C:\\Users\\dbell\\Data Science [Github]/chromedriver.exe\")\n",
    "\n",
    "#home comp\n",
    "#driver_path = webdriver.Chrome(\"C:\\Users\\Duncan\\Data Science\\chromedriver.exe\")\n",
    "\n",
    "#collecting primary articles\n",
    "stories = []\n",
    "def getTheGoodStuff(newsstories):\n",
    "    global stories\n",
    "    global i\n",
    "    for data in newsstories:\n",
    "        htmlatag = data.find(\"h2\", class_=\"title\").find(\"a\")\n",
    "        headline = htmlatag.getText()\n",
    "        url = htmlatag.get(\"href\")\n",
    "        d = {\"headline\" : headline,\n",
    "             \"url\" : url,\n",
    "            \"sort_rank\" : i}\n",
    "        stories.append(d)\n",
    "        for related in data.find_all(class_=\"related-item\"):\n",
    "            related_htmlatag = related.find(\"a\")\n",
    "            related_headline = related_htmlatag.getText()\n",
    "            related_url = related_htmlatag.get(\"href\")\n",
    "            r = {\"headline\" : related_headline,\n",
    "                 \"url\" : related_url,\n",
    "                 \"sort_rank\" : i+1}\n",
    "            stories.append(r)\n",
    "            \n",
    "def scrapeWebsites():\n",
    "    global stories\n",
    "    global i\n",
    "    \n",
    "    # Getting stories from Fox News.\n",
    "    foxnews = \"http://www.foxnews.com/\"\n",
    "    page = urllib.urlopen(foxnews)\n",
    "    r  = requests.get(foxnews)\n",
    "    data = r.text\n",
    "    soup = BeautifulSoup(data,\"lxml\")\n",
    "    for i in range(0, 20):\n",
    "        foundstories = soup.find_all(\"article\", class_=\"article story-\" + str(i))\n",
    "        getTheGoodStuff(foundstories)\n",
    "    \n",
    "scrapeWebsites()\n",
    "articles = pd.DataFrame(stories)\n",
    "articles.drop_duplicates(inplace=True)\n",
    "articles['date'] = datetime.today().strftime('%Y-%m-%d')\n",
    "articles['year'] = datetime.today().year\n",
    "articles['month'] = datetime.today().month\n",
    "articles['day'] = datetime.today().day\n",
    "articles['hour'] = datetime.today().hour\n",
    "articles['minute'] = datetime.today().minute\n",
    "\n",
    "fox_domains = []\n",
    "for web in articles.url:\n",
    "    domain_items = web.split('//')[-1].split('/')[0].strip('www.').split('.')\n",
    "    url_item = web.split('https://www.foxnews.com/')[-1].split('/')[0]\n",
    "    if domain_items[0] in ['video','foxbusiness','nation']:\n",
    "        fox_domains.append(domain_items[0])\n",
    "    else:\n",
    "        fox_domains.append(url_item)\n",
    "articles['article_type'] = fox_domains\n",
    "\n",
    "#collecting Opinion articles\n",
    "sidebar_stories = []\n",
    "\n",
    "def opinion_getTheGoodStuff(newsstories):\n",
    "    global sidebar_stories\n",
    "    for data in newsstories:\n",
    "        htmlatag = data.find(\"h2\", class_=\"title\").find(\"a\")\n",
    "        headline = htmlatag.getText()\n",
    "        url = htmlatag.get(\"href\")\n",
    "        d = {\"headline\" : headline,\n",
    "             \"url\" : url}\n",
    "        sidebar_stories.append(d)\n",
    "\n",
    "def opinion_scrapeWebsites():\n",
    "    global sidebar_stories\n",
    "    foxnews = \"http://www.foxnews.com/\"\n",
    "    page = urllib.urlopen(foxnews)\n",
    "    r  = requests.get(foxnews)\n",
    "    data = r.text\n",
    "    soup = BeautifulSoup(data,\"lxml\")\n",
    "    foundstories = soup.find(class_=\"collection collection-opinion has-load-more js-opinion\").find_all(\"article\")\n",
    "    for article in foundstories:\n",
    "        opinion_getTheGoodStuff(foundstories)\n",
    "        \n",
    "opinion_scrapeWebsites()\n",
    "sidebar_articles = pd.DataFrame(sidebar_stories)\n",
    "sidebar_articles.drop_duplicates(inplace=True)\n",
    "sort_ranks = range(sidebar_articles.shape[0]+1)\n",
    "del sort_ranks[0]\n",
    "sidebar_articles[\"sort_rank\"] = sort_ranks\n",
    "sidebar_articles['date'] = datetime.today().strftime('%Y-%m-%d')\n",
    "sidebar_articles['year'] = datetime.today().year\n",
    "sidebar_articles['month'] = datetime.today().month\n",
    "sidebar_articles['day'] = datetime.today().day\n",
    "sidebar_articles['hour'] = datetime.today().hour\n",
    "sidebar_articles['minute'] = datetime.today().minute\n",
    "\n",
    "sidebar_articles['article_type'] = \"opinion\"\n",
    "\n",
    "all_articles = pd.concat([articles,sidebar_articles],sort=False)\n",
    "\n",
    "#harvesting article text\n",
    "article_text = []\n",
    "\n",
    "fox_driver = webdriver.Chrome(\"C:\\Users\\Duncan\\Data Science\\chromedriver.exe\")\n",
    "\n",
    "for url in all_articles.url:\n",
    "    fox_driver.get(url)\n",
    "    sleep(1) #wait 1 second\n",
    "    html = fox_driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "    article_text.append(soup.find_all('p'))\n",
    "\n",
    "fox_driver.close()    \n",
    "all_articles['article_text'] = article_text\n",
    "\n",
    "foxnews_articles = pd.read_csv('foxnews.csv')\n",
    "print \"foxnews articles current shape:\"\n",
    "print foxnews_articles.shape[0]\n",
    "print \"\"\n",
    "conc = pd.concat([foxnews_articles,all_articles],sort=False)\n",
    "conc.reset_index(inplace=True)\n",
    "conc.drop(['index','Unnamed: 0'],axis=1,inplace=True)\n",
    "\n",
    "conc.to_csv('foxnews.csv', encoding = 'utf-8')\n",
    "print \"foxnews new shape:\"\n",
    "print conc.shape[0]\n",
    "\n",
    "##--------------------------------------CNN scraper----------------------------------------------\n",
    "url = \"https://www.cnn.com/\"\n",
    "html = urllib.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "\n",
    "#some code to grab the relevant chunk of html containing articles\n",
    "scripts = list(soup.find_all(\"script\"))\n",
    "n=0\n",
    "for script in scripts:\n",
    "    if n==0:\n",
    "        longest_script = script\n",
    "        n=1\n",
    "    else:\n",
    "        if len(str(script)) > len(str(longest_script)):\n",
    "            longest_script = script\n",
    "        \n",
    "cnn_content = longest_script.string\n",
    "\n",
    "#distilling down to the list of dictionaries containing article content\n",
    "cnn_list = []\n",
    "for item in cnn_content.split('{\"articleList\":'):\n",
    "    for split in item.split(']}'):\n",
    "        cnn_list.append(split)\n",
    "\n",
    "cnn_articles = []        \n",
    "for stuff in cnn_list:\n",
    "    if stuff[:8] == '[{\"uri\":':\n",
    "        cnn_articles.append(stuff.encode(\"utf-8\"))\n",
    "\n",
    "articles = cnn_articles[0].replace(\"'[\",\"\") + \"]\"\n",
    "\n",
    "article_data = json.loads(articles)\n",
    "\n",
    "data = []\n",
    "\n",
    "for article in article_data:\n",
    "    article_dict = dict(article)\n",
    "    data.append([article_dict['headline'],article_dict['description'],article_dict['uri'].encode(\"utf-8\"),article_dict['duration'],article_dict['layout'],article_dict['thumbnail']])\n",
    "    \n",
    "df = pd.DataFrame(data,columns=['headline','description','url','duration','layout','thumbnail'])\n",
    "\n",
    "df['date'] = datetime.today().strftime('%Y-%m-%d')\n",
    "df['year'] = datetime.today().year\n",
    "df['month'] = datetime.today().month\n",
    "df['day'] = datetime.today().day\n",
    "df['hour'] = datetime.today().hour\n",
    "df['minute'] = datetime.today().minute\n",
    "\n",
    "sort_ranks = []\n",
    "\n",
    "for index, record in df.iterrows():\n",
    "    if record['headline'][:8] == '<strong>':\n",
    "        sort_ranks.append(1)\n",
    "    else:\n",
    "        sort_ranks.append(sort_ranks[-1]+1)\n",
    "        \n",
    "df['sort_rank'] = sort_ranks\n",
    "\n",
    "article_types = []\n",
    "\n",
    "for index, record in df.iterrows():\n",
    "    if record['duration'] <> '':\n",
    "        article_types.append(record['url'].strip('/videos/').split('/')[0])\n",
    "    else:\n",
    "        article_types.append(record['url'].translate(None, digits).strip('////').split('/')[0])\n",
    "\n",
    "df['article_type'] = article_types\n",
    "\n",
    "#harvesting article text\n",
    "article_text = []\n",
    "\n",
    "cnn_driver = webdriver.Chrome(\"C:\\Users\\Duncan\\Data Science\\chromedriver.exe\")\n",
    "\n",
    "for url in df.url:\n",
    "    cnn_driver.get('https://www.cnn.com' + url)\n",
    "    sleep(1) #wait 1 second\n",
    "    html = cnn_driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "    article_text.append(soup.find_all(class_=re.compile(\"paragraph\")))\n",
    "\n",
    "cnn_driver.close()    \n",
    "df['article_text'] = article_text\n",
    "\n",
    "cnn_articles = pd.read_csv('cnn.csv')\n",
    "print \"cnn articles current shape:\"\n",
    "print cnn_articles.shape[0]\n",
    "print \"\"\n",
    "conc = pd.concat([cnn_articles,df],sort=False)\n",
    "conc.reset_index(inplace=True)\n",
    "conc.drop(['index','Unnamed: 0'],axis=1,inplace=True)\n",
    "\n",
    "conc.to_csv('cnn.csv', encoding = 'utf-8')\n",
    "print \"cnn new shape:\"\n",
    "print conc.shape[0]\n",
    "\n",
    "#-----------------------------------------MSNBC Scraper----------------------------------------------------\n",
    "\n",
    "stories = []\n",
    "i = 0\n",
    "def getTheGoodStuff(newsstories):\n",
    "    global stories\n",
    "    global i\n",
    "    if newsstories.find(\"a\") <> None:\n",
    "        i=i+1\n",
    "        htmlatag = newsstories.find(\"a\")\n",
    "        url = htmlatag.get(\"href\")\n",
    "        headline = htmlatag.getText()\n",
    "        d = {\"headline\" : headline,\n",
    "             \"url\" : url,\n",
    "             \"sort_rank\" : i}\n",
    "        stories.append(d)\n",
    "            \n",
    "def scrapeMSNBC():\n",
    "    global stories\n",
    "    global newsstories\n",
    "    global i\n",
    "    # Getting stories from MSNBC\n",
    "    msnbc = \"https://www.msnbc.com/\"\n",
    "    page = urllib.urlopen(msnbc)\n",
    "    r  = requests.get(msnbc)\n",
    "    data = r.text\n",
    "    soup = BeautifulSoup(data,\"lxml\")\n",
    "    for htmlstuff in soup.find_all(class_=re.compile(\"title\")):\n",
    "        getTheGoodStuff(htmlstuff)\n",
    "        \n",
    "scrapeMSNBC()\n",
    "\n",
    "#grabbing \"related\" type stories using very similar approach\n",
    "def getrelated(newsstories):\n",
    "    global stories\n",
    "    if newsstories.find(\"a\") <> None:\n",
    "        htmlatag = newsstories.find(\"a\")\n",
    "        url = htmlatag.get(\"href\")\n",
    "        headline = htmlatag.getText()\n",
    "        d = {\"headline\" : headline,\n",
    "             \"url\" : url,\n",
    "             \"sort_rank\" : 3}\n",
    "        stories.append(d)\n",
    "            \n",
    "def scrapeMSNBC_related():\n",
    "    global stories\n",
    "    global newsstories\n",
    "    # Getting stories from MSNBC\n",
    "    msnbc = \"https://www.msnbc.com/\"\n",
    "    page = urllib.urlopen(msnbc)\n",
    "    r  = requests.get(msnbc)\n",
    "    data = r.text\n",
    "    soup = BeautifulSoup(data,\"lxml\")\n",
    "    for htmlstuff in soup.find_all(class_=re.compile(\"related\")):\n",
    "        getrelated(htmlstuff)\n",
    "                                   \n",
    "scrapeMSNBC_related()\n",
    "articles = pd.DataFrame(stories)\n",
    "articles.drop_duplicates(inplace=True)\n",
    "articles['date'] = datetime.today().strftime('%Y-%m-%d')\n",
    "articles['year'] = datetime.today().year\n",
    "articles['month'] = datetime.today().month\n",
    "articles['day'] = datetime.today().day\n",
    "articles['hour'] = datetime.today().hour\n",
    "articles['minute'] = datetime.today().minute\n",
    "\n",
    "domains = []\n",
    "for web in articles.url:\n",
    "    domain_items = web.split('//')[-1].split('/')[0].strip('www.').split('.')\n",
    "    url_item = web.split('https://www.msnbc.com/msnbc/')[-1].split('https://www.msnbc.com/')[-1].split('http://www.msnbc.com/')[-1].split('https://www.nbcnews.com/politics/')[-1].split('https://www.nbcnews.com/news/')[-1].split('https://www.nbcnews.com/tech/')[-1].split('https://www.nbcnews.com/better/')[-1].split('https://www.nbcnews.com/mach/')[-1].split('https://www.nbcnews.com/think/')[-1].split('https://www.nbcnews.com/feature/')[-1].split('https://www.nbcnews.com/')[-1].split('https://podcasts.apple.com/')[-1].split('/')[0]\n",
    "    domains.append(url_item)\n",
    "articles['domain'] = domains\n",
    "\n",
    "#Now that we have the article URLs, now we use chrome driver to fetch text from each article\n",
    "msnbc_text = []\n",
    "\n",
    "msnbc_driver = webdriver.Chrome(\"C:\\Users\\Duncan\\Data Science\\chromedriver.exe\")\n",
    "\n",
    "for url in articles.url:\n",
    "    msnbc_driver.get(url)\n",
    "    sleep(1) #wait 1 second\n",
    "    html = msnbc_driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "    msnbc_text.append(soup.find_all('p'))\n",
    "\n",
    "msnbc_driver.close()    \n",
    "articles['article_text'] = msnbc_text\n",
    "\n",
    "msnbc_articles = pd.read_csv('msnbc.csv')\n",
    "print \"msnbc articles current shape:\"\n",
    "print msnbc_articles.shape[0]\n",
    "print \"\"\n",
    "conc = pd.concat([msnbc_articles,articles],sort=False)\n",
    "conc.reset_index(inplace=True)\n",
    "conc.drop(['index','Unnamed: 0'],axis=1,inplace=True)\n",
    "\n",
    "conc.to_csv('msnbc.csv', encoding = 'utf-8')\n",
    "print \"msnbc new shape:\"\n",
    "print conc.shape[0]\n",
    "print \"\"\n",
    "\n",
    "print \"Last run:\"\n",
    "print datetime.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn articles current shape:\n",
      "2148\n",
      "\n",
      "cnn new shape:\n",
      "2208\n",
      "\n",
      "Last run:\n",
      "2019-06-28 18:23:23.606000\n"
     ]
    }
   ],
   "source": [
    "#run CNN only\n",
    "url = \"https://www.cnn.com/\"\n",
    "html = urllib.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "\n",
    "#some code to grab the relevant chunk of html containing articles\n",
    "scripts = list(soup.find_all(\"script\"))\n",
    "n=0\n",
    "for script in scripts:\n",
    "    if n==0:\n",
    "        longest_script = script\n",
    "        n=1\n",
    "    else:\n",
    "        if len(str(script)) > len(str(longest_script)):\n",
    "            longest_script = script\n",
    "        \n",
    "cnn_content = longest_script.string\n",
    "\n",
    "#distilling down to the list of dictionaries containing article content\n",
    "cnn_list = []\n",
    "for item in cnn_content.split('{\"articleList\":'):\n",
    "    for split in item.split(']}'):\n",
    "        cnn_list.append(split)\n",
    "\n",
    "cnn_articles = []        \n",
    "for stuff in cnn_list:\n",
    "    if stuff[:8] == '[{\"uri\":':\n",
    "        cnn_articles.append(stuff.encode(\"utf-8\"))\n",
    "\n",
    "articles = cnn_articles[0].replace(\"'[\",\"\") + \"]\"\n",
    "\n",
    "article_data = json.loads(articles)\n",
    "\n",
    "data = []\n",
    "\n",
    "for article in article_data:\n",
    "    article_dict = dict(article)\n",
    "    data.append([article_dict['headline'],article_dict['description'],article_dict['uri'].encode(\"utf-8\"),article_dict['duration'],article_dict['layout'],article_dict['thumbnail']])\n",
    "    \n",
    "df = pd.DataFrame(data,columns=['headline','description','url','duration','layout','thumbnail'])\n",
    "\n",
    "df['date'] = datetime.today().strftime('%Y-%m-%d')\n",
    "df['year'] = datetime.today().year\n",
    "df['month'] = datetime.today().month\n",
    "df['day'] = datetime.today().day\n",
    "df['hour'] = datetime.today().hour\n",
    "df['minute'] = datetime.today().minute\n",
    "\n",
    "sort_ranks = []\n",
    "\n",
    "for index, record in df.iterrows():\n",
    "    if record['headline'][:8] == '<strong>':\n",
    "        sort_ranks.append(1)\n",
    "    else:\n",
    "        sort_ranks.append(sort_ranks[-1]+1)\n",
    "        \n",
    "df['sort_rank'] = sort_ranks\n",
    "\n",
    "article_types = []\n",
    "\n",
    "for index, record in df.iterrows():\n",
    "    if record['duration'] <> '':\n",
    "        article_types.append(record['url'].strip('/videos/').split('/')[0])\n",
    "    else:\n",
    "        article_types.append(record['url'].translate(None, digits).strip('////').split('/')[0])\n",
    "\n",
    "df['article_type'] = article_types\n",
    "\n",
    "#harvesting article text\n",
    "article_text = []\n",
    "\n",
    "cnn_driver = webdriver.Chrome(\"C:\\Users\\Duncan\\Data Science\\chromedriver.exe\")\n",
    "\n",
    "for url in df.url:\n",
    "    cnn_driver.get('https://www.cnn.com' + url)\n",
    "    sleep(1) #wait 1 second\n",
    "    html = cnn_driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "    article_text.append(soup.find_all(class_=re.compile(\"paragraph\")))\n",
    "\n",
    "cnn_driver.close()    \n",
    "df['article_text'] = article_text\n",
    "\n",
    "cnn_articles = pd.read_csv('cnn.csv')\n",
    "print \"cnn articles current shape:\"\n",
    "print cnn_articles.shape[0]\n",
    "print \"\"\n",
    "conc = pd.concat([cnn_articles,df],sort=False)\n",
    "conc.reset_index(inplace=True)\n",
    "conc.drop(['index','Unnamed: 0'],axis=1,inplace=True)\n",
    "\n",
    "conc.to_csv('cnn.csv', encoding = 'utf-8')\n",
    "print \"cnn new shape:\"\n",
    "print conc.shape[0]\n",
    "print \"\"\n",
    "\n",
    "print \"Last run:\"\n",
    "print datetime.today()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
