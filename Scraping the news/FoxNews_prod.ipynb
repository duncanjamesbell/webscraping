{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "#collecting primary articles\n",
    "stories = []\n",
    "def getTheGoodStuff(newsstories):\n",
    "    global stories\n",
    "    global i\n",
    "    for data in newsstories:\n",
    "        htmlatag = data.find(\"h2\", class_=\"title\").find(\"a\")\n",
    "        headline = htmlatag.getText()\n",
    "        url = htmlatag.get(\"href\")\n",
    "        d = {\"headline\" : headline,\n",
    "             \"url\" : url,\n",
    "            \"sort_rank\" : i}\n",
    "        stories.append(d)\n",
    "        for related in data.find_all(class_=\"related-item\"):\n",
    "            related_htmlatag = related.find(\"a\")\n",
    "            related_headline = related_htmlatag.getText()\n",
    "            related_url = related_htmlatag.get(\"href\")\n",
    "            r = {\"headline\" : related_headline,\n",
    "                 \"url\" : related_url,\n",
    "                 \"sort_rank\" : i+1}\n",
    "            stories.append(r)\n",
    "            \n",
    "def scrapeWebsites():\n",
    "    global stories\n",
    "    global i\n",
    "    \n",
    "    # Getting stories from Fox News.\n",
    "    foxnews = \"http://www.foxnews.com/\"\n",
    "    page = urllib.urlopen(foxnews)\n",
    "    r  = requests.get(foxnews)\n",
    "    data = r.text\n",
    "    soup = BeautifulSoup(data,\"lxml\")\n",
    "    for i in range(0, 20):\n",
    "        foundstories = soup.find_all(\"article\", class_=\"article story-\" + str(i))\n",
    "        getTheGoodStuff(foundstories)\n",
    "    \n",
    "scrapeWebsites()\n",
    "articles = pd.DataFrame(stories)\n",
    "articles.drop_duplicates(inplace=True)\n",
    "articles['date'] = datetime.today().strftime('%Y-%m-%d')\n",
    "articles['year'] = datetime.today().year\n",
    "articles['month'] = datetime.today().month\n",
    "articles['day'] = datetime.today().day\n",
    "articles['hour'] = datetime.today().hour\n",
    "articles['minute'] = datetime.today().minute\n",
    "\n",
    "fox_domains = []\n",
    "for web in articles.url:\n",
    "    domain_items = web.split('//')[-1].split('/')[0].strip('www.').split('.')\n",
    "    url_item = web.split('https://www.foxnews.com/')[-1].split('/')[0]\n",
    "    if domain_items[0] in ['video','foxbusiness','nation']:\n",
    "        fox_domains.append(domain_items[0])\n",
    "    else:\n",
    "        fox_domains.append(url_item)\n",
    "articles['article_type'] = fox_domains\n",
    "\n",
    "#collecting Opinion articles\n",
    "sidebar_stories = []\n",
    "\n",
    "def opinion_getTheGoodStuff(newsstories):\n",
    "    global sidebar_stories\n",
    "    for data in newsstories:\n",
    "        htmlatag = data.find(\"h2\", class_=\"title\").find(\"a\")\n",
    "        headline = htmlatag.getText()\n",
    "        url = htmlatag.get(\"href\")\n",
    "        d = {\"headline\" : headline,\n",
    "             \"url\" : url}\n",
    "        sidebar_stories.append(d)\n",
    "\n",
    "def opinion_scrapeWebsites():\n",
    "    global sidebar_stories\n",
    "    foxnews = \"http://www.foxnews.com/\"\n",
    "    page = urllib.urlopen(foxnews)\n",
    "    r  = requests.get(foxnews)\n",
    "    data = r.text\n",
    "    soup = BeautifulSoup(data,\"lxml\")\n",
    "    foundstories = soup.find(class_=\"collection collection-opinion has-load-more js-opinion\").find_all(\"article\")\n",
    "    for article in foundstories:\n",
    "        opinion_getTheGoodStuff(foundstories)\n",
    "        \n",
    "opinion_scrapeWebsites()\n",
    "sidebar_articles = pd.DataFrame(sidebar_stories)\n",
    "sidebar_articles.drop_duplicates(inplace=True)\n",
    "sort_ranks = range(sidebar_articles.shape[0]+1)\n",
    "del sort_ranks[0]\n",
    "sidebar_articles[\"sort_rank\"] = sort_ranks\n",
    "sidebar_articles['date'] = datetime.today().strftime('%Y-%m-%d')\n",
    "sidebar_articles['year'] = datetime.today().year\n",
    "sidebar_articles['month'] = datetime.today().month\n",
    "sidebar_articles['day'] = datetime.today().day\n",
    "sidebar_articles['hour'] = datetime.today().hour\n",
    "sidebar_articles['minute'] = datetime.today().minute\n",
    "\n",
    "sidebar_articles['article_type'] = \"opinion\"\n",
    "\n",
    "all_articles = pd.concat([articles,sidebar_articles],sort=False)\n",
    "\n",
    "#harvesting article text\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "\n",
    "article_text = []\n",
    "driver = webdriver.Chrome(\"C:\\Users\\Duncan\\Data Science\\chromedriver.exe\")\n",
    "\n",
    "for url in all_articles.url:\n",
    "    driver.get(url)\n",
    "    sleep(1) #wait 1 second\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "    article_text.append(soup.find_all('p'))\n",
    "\n",
    "driver.close()    \n",
    "all_articles['article_text'] = article_text\n",
    "\n",
    "foxnews_articles = pd.read_csv('foxnews.csv')\n",
    "print \"foxnews articles current shape:\"\n",
    "print foxnews_articles.shape[0]\n",
    "print \"\"\n",
    "conc = pd.concat([foxnews_articles,all_articles],sort=False)\n",
    "conc.reset_index(inplace=True)\n",
    "conc.drop(['index','Unnamed: 0'],axis=1,inplace=True)\n",
    "\n",
    "conc.to_csv('foxnews.csv', encoding = 'utf-8')\n",
    "print \"foxnews new shape:\"\n",
    "print conc.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
