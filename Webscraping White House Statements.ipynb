{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import our necessary first packages\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the url we want to visit\n",
    "url = \"https://www.whitehouse.gov/news\"\n",
    "\n",
    "# visit that url, and grab the html of said page\n",
    "html = urllib.urlopen(url).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html lang=\"en-US\" prefix=\"og: http://ogp.me/ns# fb: http://ogp.me/ns/fb#\" class=\"no-js\">\\n<head>\\n\\t<script>\\n\\t\\tdocument.documentElement.className = document.documentElement.className.replace(/\\\\bno-js\\\\b/, \\'js\\');\\n\\t\\tdocument.createElement(\\'picture\\');\\n\\t</script>\\n\\t<meta charset=\"UTF-8\">\\n\\t<meta http-equiv=\"x-ua-compatible\" content=\"ie=edge\"><script type=\"text/javascript\">(window.NREUM||(NREUM={})).loader_config={xpid:\"VQ4FUVdTCBAIVVFUBQEEVw==\"};window.NREUM||(NREUM={}),__nr_require=function(t,n,e){function r(e){if(!n[e]){var o=n[e]={exports:{}};t[e][0].call(o.exports,function(n){var o=t[e][1][n];return r(o||n)},o,o.exports)}return n[e].exports}if(\"function\"==typeof __nr_require)return __nr_require;for(var o=0;o<e.length;o++)r(e[o]);return r}({1:[function(t,n,e){function r(t){try{s.console&&console.log(t)}catch(n){}}var o,i=t(\"ee\"),a=t(16),s={};try{o=localStorage.getItem(\"__nr_flags\").split(\",\"),console&&\"function\"==typeof console.log&&(s.console=!0,o.indexOf(\"dev\")!==-1&&(s.dev=!0),o.indexOf(\"nr_dev\")!==-1&&(s.nrDev=!0))}catch(c){}s.nrDev&&i.on(\"internal-error\",function(t){r(t.stack)}),s.dev&&i.on(\"fn-err\",function(t,n,e){r(e.stack)}),s.dev&&(r(\"NR AGENT IN DEVELOPMENT MODE\"),r(\"flags: \"+a(s,function(t,n){return t}).join(\", \")))},{}],2:[function(t,n,e){function r(t,n,e,r,s){try{p?p-=1:o(s||new UncaughtException(t,n,e),!0)}catch(f){try{i(\"ierr\",[f,c.now(),!0])}catch(d){}}return\"function\"==typeof u&&u.apply(this,a(arguments))}function UncaughtException(t,n,e){this.message=t||\"Uncaught error with no additional information\",this.sourceURL=n,this.line=e}function o(t,n){var e=n?null:c.now();i(\"err\",[t,e])}var i=t(\"handle\"),a=t(17),s=t(\"ee\"),c=t(\"loader\"),f=t(\"gos\"),u=window.onerror,d=!1,l=\"nr@seenError\",p=0;c.features.err=!0,t(1),window.onerror=r;try{throw new Error}catch(h){\"stack\"in h&&(t(8),t(7),\"addEventListener\"in window&&t(5),c.xhrWrappable&&t(9),d=!0)}s.on(\"fn-start\",function(t,n,e){d&&(p+=1)}),s.on(\"fn-err\",function(t,n,e){d&&!e[l]&&(f(e,l,function(){return!0}),this.thrown=!0,o(e))}),s.on(\"fn-end\",function(){d&&!this.thrown&&p>0&&(p-=1)}),s.on(\"internal-error\",function(t){i(\"ierr\",[t,c.now(),!0])})},{}],3:[function(t,n,e){t(\"loader\").features.ins=!0},{}],4:[function(t,n,e){function r(t){}if(window.performance&&window.performance.timing&&window.performance.getEntriesByType){var o=t(\"ee\"),i=t(\"handle\"),a=t(8),s=t(7),c=\"learResourceTimings\",f=\"addEventListener\",u=\"resourcetimingbufferfull\",d=\"bstResource\",l=\"resource\",p=\"-start\",h=\"-end\",m=\"fn\"+p,v=\"fn\"+h,w=\"bstTimer\",y=\"pushState\",g=t(\"loader\");g.features.stn=!0,t(6);var b=NREUM.o.EV;o.on(m,function(t,n){var e=t[0];e instanceof b&&(this.bstStart=g.now())}),o.on(v,function(t,n){var e=t[0];e instanceof b&&i(\"bst\",[e,n,this.bstStart,g.now()])}),a.on(m,function(t,n,e){this.bstStart=g.now(),this.bstType=e}),a.on(v,function(t,n){i(w,[n,this.bstStart,g.now(),this.bstType])}),s.on(m,function(){this.bstStart=g.now()}),s.on(v,function(t,n){i(w,[n,this.bstStart,g.now(),\"requestAnimationFrame\"])}),o.on(y+p,function(t){this.time=g.now(),this.startPath=location.pathname+location.hash}),o.on(y+h,function(t){i(\"bstHist\",[location.pathname+location.hash,this.startPath,this.time])}),f in window.performance&&(window.performance[\"c\"+c]?window.performance[f](u,function(t){i(d,[window.performance.getEntriesByType(l)]),window.performance[\"c\"+c]()},!1):window.performance[f](\"webkit\"+u,function(t){i(d,[window.performance.getEntriesByType(l)]),window.performance[\"webkitC\"+c]()},!1)),document[f](\"scroll\",r,{passive:!0}),document[f](\"keypress\",r,!1),document[f](\"click\",r,!1)}},{}],5:[function(t,n,e){function r(t){for(var n=t;n&&!n.hasOwnProperty(u);)n=Object.getPrototypeOf(n);n&&o(n)}function o(t){s.inPlace(t,[u,d],\"-\",i)}function i(t,n){return t[1]}var a=t(\"ee\").get(\"events\"),s=t(19)(a,!0),c=t(\"gos\"),f=XMLHttpRequest,u=\"addEventListener\",d=\"removeEventListener\";n.exports=a,\"getPrototypeOf\"in Object?(r(document),r(window),r(f.prototype)):f.prototype.hasOwnProperty(u)&&(o(window),o(f.prototype)),a.on(u+\"-start\",function(t,n){var e=t[1],r=c(e,\"nr@wrapped\",function(){function t(){if(\"function\"==typeof e.handleEvent)return e.handleEvent.apply(e,arguments)}var n={object:t,\"function\":e}[typeof e];return n?s(n,\"fn-\",null,n.name||\"anonymous\"):e});this.wrapped=t[1]=r}),a.on(d+\"-start\",function(t){t[1]=this.wrapped||t[1]})},{}],6:[function(t,n,e){var r=t(\"ee\").get(\"history\"),o=t(19)(r);n.exports=r,o.inPlace(window.history,[\"pushState\",\"replaceState\"],\"-\")},{}],7:[function(t,n,e){var r=t(\"ee\").get(\"raf\"),o=t(19)(r),i=\"equestAnimationFrame\";n.exports=r,o.inPlace(window,[\"r\"+i,\"mozR\"+i,\"webkitR\"+i,\"msR\"+i],\"raf-\"),r.on(\"raf-start\",function(t){t[0]=o(t[0],\"fn-\")})},{}],8:[function(t,n,e){function r(t,n,e){t[0]=a(t[0],\"fn-\",null,e)}function o(t,n,e){this.method=e,this.timerDuration=isNaN(t[1])?0:+t[1],t[0]=a(t[0],\"fn-\",this,e)}var i=t(\"ee\").get(\"timer\"),a=t(19)(i),s=\"setTimeout\",c=\"setInterval\",f=\"clearTimeout\",u=\"-start\",d=\"-\";n.exports=i,a.inPlace(window,[s,\"setImmediate\"],s+d),a.inPlace(window,[c],c+d),a.inPlace(window,[f,\"clearImmediate\"],f+d),i.on(c+u,r),i.on(s+u,o)},{}],9:[function(t,n,e){function r(t,n){d.inPlace(n,[\"onreadystatechange\"],\"fn-\",s)}function o(){var t=this,n=u.context(t);t.readyState>3&&!n.resolved&&(n.resolved=!0,u.emit(\"xhr-resolved\",[],t)),d.inPlace(t,y,\"fn-\",s)}function i(t){g.push(t),h&&(x?x.then(a):v?v(a):(E=-E,O.data=E))}function a(){for(var t=0;t<g.length;t++)r([],g[t]);g.length&&(g=[])}function s(t,n){return n}function c(t,n){for(var e in t)n[e]=t[e];return n}t(5);var f=t(\"ee\"),u=f.get(\"xhr\"),d=t(19)(u),l=NREUM.o,p=l.XHR,h=l.MO,m=l.PR,v=l.SI,w=\"readystatechange\",y=[\"onload\",\"onerror\",\"onabort\",\"onloadstart\",\"onloadend\",\"onprogress\",\"ontimeout\"],g=[];n.exports=u;var b=window.XMLHttpRequest=function(t){var n=new p(t);try{u.emit(\"new-xhr\",[n],n),n.addEventListener(w,o,!1)}catch(e){try{u.emit(\"internal-error\",[e])}catch(r){}}return n};if(c(p,b),b.prototype=p.prototype,d.inPlace(b.prototype,[\"open\",\"send\"],\"-xhr-\",s),u.on(\"send-xhr-start\",function(t,n){r(t,n),i(n)}),u.on(\"open-xhr-start\",r),h){var x=m&&m.resolve();if(!v&&!m){var E=1,O=document.createTextNode(E);new h(a).observe(O,{characterData:!0})}}else f.on(\"fn-end\",function(t){t[0]&&t[0].type===w||a()})},{}],10:[function(t,n,e){function r(t){var n=this.params,e=this.metrics;if(!this.ended){this.ended=!0;for(var r=0;r<d;r++)t.removeEventListener(u[r],this.listener,!1);if(!n.aborted){if(e.duration=a.now()-this.startTime,4===t.readyState){n.status=t.status;var i=o(t,this.lastSize);if(i&&(e.rxSize=i),this.sameOrigin){var c=t.getResponseHeader(\"X-NewRelic-App-Data\");c&&(n.cat=c.split(\", \").pop())}}else n.status=0;e.cbTime=this.cbTime,f.emit(\"xhr-done\",[t],t),s(\"xhr\",[n,e,this.startTime])}}}function o(t,n){var e=t.responseType;if(\"json\"===e&&null!==n)return n;var r=\"arraybuffer\"===e||\"blob\"===e||\"json\"===e?t.response:t.responseText;return h(r)}function i(t,n){var e=c(n),r=t.params;r.host=e.hostname+\":\"+e.port,r.pathname=e.pathname,t.sameOrigin=e.sameOrigin}var a=t(\"loader\");if(a.xhrWrappable){var s=t(\"handle\"),c=t(11),f=t(\"ee\"),u=[\"load\",\"error\",\"abort\",\"timeout\"],d=u.length,l=t(\"id\"),p=t(14),h=t(13),m=window.XMLHttpRequest;a.features.xhr=!0,t(9),f.on(\"new-xhr\",function(t){var n=this;n.totalCbs=0,n.called=0,n.cbTime=0,n.end=r,n.ended=!1,n.xhrGuids={},n.lastSize=null,p&&(p>34||p<10)||window.opera||t.addEventListener(\"progress\",function(t){n.lastSize=t.loaded},!1)}),f.on(\"open-xhr-start\",function(t){this.params={method:t[0]},i(this,t[1]),this.metrics={}}),f.on(\"open-xhr-end\",function(t,n){\"loader_config\"in NREUM&&\"xpid\"in NREUM.loader_config&&this.sameOrigin&&n.setRequestHeader(\"X-NewRelic-ID\",NREUM.loader_config.xpid)}),f.on(\"send-xhr-start\",function(t,n){var e=this.metrics,r=t[0],o=this;if(e&&r){var i=h(r);i&&(e.txSize=i)}this.startTime=a.now(),this.listener=function(t){try{\"abort\"===t.type&&(o.params.aborted=!0),(\"load\"!==t.type||o.called===o.totalCbs&&(o.onloadCalled||\"function\"!=typeof n.onload))&&o.end(n)}catch(e){try{f.emit(\"internal-error\",[e])}catch(r){}}};for(var s=0;s<d;s++)n.addEventListener(u[s],this.listener,!1)}),f.on(\"xhr-cb-time\",function(t,n,e){this.cbTime+=t,n?this.onloadCalled=!0:this.called+=1,this.called!==this.totalCbs||!this.onloadCalled&&\"function\"==typeof e.onload||this.end(e)}),f.on(\"xhr-load-added\",function(t,n){var e=\"\"+l(t)+!!n;this.xhrGuids&&!this.xhrGuids[e]&&(this.xhrGuids[e]=!0,this.totalCbs+=1)}),f.on(\"xhr-load-removed\",function(t,n){var e=\"\"+l(t)+!!n;this.xhrGuids&&this.xhrGuids[e]&&(delete this.xhrGuids[e],this.totalCbs-=1)}),f.on(\"addEventListener-end\",function(t,n){n instanceof m&&\"load\"===t[0]&&f.emit(\"xhr-load-added\",[t[1],t[2]],n)}),f.on(\"removeEventListener-end\",function(t,n){n instanceof m&&\"load\"===t[0]&&f.emit(\"xhr-load-removed\",[t[1],t[2]],n)}),f.on(\"fn-start\",function(t,n,e){n instanceof m&&(\"onload\"===e&&(this.onload=!0),(\"load\"===(t[0]&&t[0].type)||this.onload)&&(this.xhrCbStart=a.now()))}),f.on(\"fn-end\",function(t,n){this.xhrCbStart&&f.emit(\"xhr-cb-time\",[a.now()-this.xhrCbStart,this.onload,n],n)})}},{}],11:[function(t,n,e){n.exports=function(t){var n=document.createElement(\"a\"),e=window.location,r={};n.href=t,r.port=n.port;var o=n.href.split(\"://\");!r.port&&o[1]&&(r.port=o[1].split(\"/\")[0].split(\"@\").pop().split(\":\")[1]),r.port&&\"0\"!==r.port||(r.port=\"https\"===o[0]?\"443\":\"80\"),r.hostname=n.hostname||e.hostname,r.pathname=n.pathname,r.protocol=o[0],\"/\"!==r.pathname.charAt(0)&&(r.pathname=\"/\"+r.pathname);var i=!n.protocol||\":\"===n.protocol||n.protocol===e.protocol,a=n.hostname===document.domain&&n.port===e.port;return r.sameOrigin=i&&(!n.hostname||a),r}},{}],12:[function(t,n,e){function r(){}function o(t,n,e){return function(){return i(t,[f.now()].concat(s(arguments)),n?null:this,e),n?void 0:this}}var i=t(\"handle\"),a=t(16),s=t(17),c=t(\"ee\").get(\"tracer\"),f=t(\"loader\"),u=NREUM;\"undefined\"==typeof window.newrelic&&(newrelic=u);var d=[\"setPageViewName\",\"setCustomAttribute\",\"setErrorHandler\",\"finished\",\"addToTrace\",\"inlineHit\",\"addRelease\"],l=\"api-\",p=l+\"ixn-\";a(d,function(t,n){u[n]=o(l+n,!0,\"api\")}),u.addPageAction=o(l+\"addPageAction\",!0),u.setCurrentRouteName=o(l+\"routeName\",!0),n.exports=newrelic,u.interaction=function(){return(new r).get()};var h=r.prototype={createTracer:function(t,n){var e={},r=this,o=\"function\"==typeof n;return i(p+\"tracer\",[f.now(),t,e],r),function(){if(c.emit((o?\"\":\"no-\")+\"fn-start\",[f.now(),r,o],e),o)try{return n.apply(this,arguments)}catch(t){throw c.emit(\"fn-err\",[arguments,this,t],e),t}finally{c.emit(\"fn-end\",[f.now()],e)}}}};a(\"actionText,setName,setAttribute,save,ignore,onEnd,getContext,end,get\".split(\",\"),function(t,n){h[n]=o(p+n)}),newrelic.noticeError=function(t){\"string\"==typeof t&&(t=new Error(t)),i(\"err\",[t,f.now()])}},{}],13:[function(t,n,e){n.exports=function(t){if(\"string\"==typeof t&&t.length)return t.length;if(\"object\"==typeof t){if(\"undefined\"!=typeof ArrayBuffer&&t instanceof ArrayBuffer&&t.byteLength)return t.byteLength;if(\"undefined\"!=typeof Blob&&t instanceof Blob&&t.size)return t.size;if(!(\"undefined\"!=typeof FormData&&t instanceof FormData))try{return JSON.stringify(t).length}catch(n){return}}}},{}],14:[function(t,n,e){var r=0,o=navigator.userAgent.match(/Firefox[\\\\/\\\\s](\\\\d+\\\\.\\\\d+)/);o&&(r=+o[1]),n.exports=r},{}],15:[function(t,n,e){function r(t,n){if(!o)return!1;if(t!==o)return!1;if(!n)return!0;if(!i)return!1;for(var e=i.split(\".\"),r=n.split(\".\"),a=0;a<r.length;a++)if(r[a]!==e[a])return!1;return!0}var o=null,i=null,a=/Version\\\\/(\\\\S+)\\\\s+Safari/;if(navigator.userAgent){var s=navigator.userAgent,c=s.match(a);c&&s.indexOf(\"Chrome\")===-1&&s.indexOf(\"Chromium\")===-1&&(o=\"Safari\",i=c[1])}n.exports={agent:o,version:i,match:r}},{}],16:[function(t,n,e){function r(t,n){var e=[],r=\"\",i=0;for(r in t)o.call(t,r)&&(e[i]=n(r,t[r]),i+=1);return e}var o=Object.prototype.hasOwnProperty;n.exports=r},{}],17:[function(t,n,e){function r(t,n,e){n||(n=0),\"undefined\"==typeof e&&(e=t?t.length:0);for(var r=-1,o=e-n||0,i=Array(o<0?0:o);++r<o;)i[r]=t[n+r];return i}n.exports=r},{}],18:[function(t,n,e){n.exports={exists:\"undefined\"!=typeof window.performance&&window.performance.timing&&\"undefined\"!=typeof window.performance.timing.navigationStart}},{}],19:[function(t,n,e){function r(t){return!(t&&t instanceof Function&&t.apply&&!t[a])}var o=t(\"ee\"),i=t(17),a=\"nr@original\",s=Object.prototype.hasOwnProperty,c=!1;n.exports=function(t,n){function e(t,n,e,o){function nrWrapper(){var r,a,s,c;try{a=this,r=i(arguments),s=\"function\"==typeof e?e(r,a):e||{}}catch(f){l([f,\"\",[r,a,o],s])}u(n+\"start\",[r,a,o],s);try{return c=t.apply(a,r)}catch(d){throw u(n+\"err\",[r,a,d],s),d}finally{u(n+\"end\",[r,a,c],s)}}return r(t)?t:(n||(n=\"\"),nrWrapper[a]=t,d(t,nrWrapper),nrWrapper)}function f(t,n,o,i){o||(o=\"\");var a,s,c,f=\"-\"===o.charAt(0);for(c=0;c<n.length;c++)s=n[c],a=t[s],r(a)||(t[s]=e(a,f?s+o:o,i,s))}function u(e,r,o){if(!c||n){var i=c;c=!0;try{t.emit(e,r,o,n)}catch(a){l([a,e,r,o])}c=i}}function d(t,n){if(Object.defineProperty&&Object.keys)try{var e=Object.keys(t);return e.forEach(function(e){Object.defineProperty(n,e,{get:function(){return t[e]},set:function(n){return t[e]=n,n}})}),n}catch(r){l([r])}for(var o in t)s.call(t,o)&&(n[o]=t[o]);return n}function l(n){try{t.emit(\"internal-error\",n)}catch(e){}}return t||(t=o),e.inPlace=f,e.flag=a,e}},{}],ee:[function(t,n,e){function r(){}function o(t){function n(t){return t&&t instanceof r?t:t?c(t,s,i):i()}function e(e,r,o,i){if(!l.aborted||i){t&&t(e,r,o);for(var a=n(o),s=m(e),c=s.length,f=0;f<c;f++)s[f].apply(a,r);var d=u[g[e]];return d&&d.push([b,e,r,a]),a}}function p(t,n){y[t]=m(t).concat(n)}function h(t,n){var e=y[t];if(e)for(var r=0;r<e.length;r++)e[r]===n&&e.splice(r,1)}function m(t){return y[t]||[]}function v(t){return d[t]=d[t]||o(e)}function w(t,n){f(t,function(t,e){n=n||\"feature\",g[e]=n,n in u||(u[n]=[])})}var y={},g={},b={on:p,addEventListener:p,removeEventListener:h,emit:e,get:v,listeners:m,context:n,buffer:w,abort:a,aborted:!1};return b}function i(){return new r}function a(){(u.api||u.feature)&&(l.aborted=!0,u=l.backlog={})}var s=\"nr@context\",c=t(\"gos\"),f=t(16),u={},d={},l=n.exports=o();l.backlog=u},{}],gos:[function(t,n,e){function r(t,n,e){if(o.call(t,n))return t[n];var r=e();if(Object.defineProperty&&Object.keys)try{return Object.defineProperty(t,n,{value:r,writable:!0,enumerable:!1}),r}catch(i){}return t[n]=r,r}var o=Object.prototype.hasOwnProperty;n.exports=r},{}],handle:[function(t,n,e){function r(t,n,e,r){o.buffer([t],r),o.emit(t,n,e)}var o=t(\"ee\").get(\"handle\");n.exports=r,r.ee=o},{}],id:[function(t,n,e){function r(t){var n=typeof t;return!t||\"object\"!==n&&\"function\"!==n?-1:t===window?0:a(t,i,function(){return o++})}var o=1,i=\"nr@id\",a=t(\"gos\");n.exports=r},{}],loader:[function(t,n,e){function r(){if(!E++){var t=x.info=NREUM.info,n=p.getElementsByTagName(\"script\")[0];if(setTimeout(u.abort,3e4),!(t&&t.licenseKey&&t.applicationID&&n))return u.abort();f(g,function(n,e){t[n]||(t[n]=e)}),c(\"mark\",[\"onload\",a()+x.offset],null,\"api\");var e=p.createElement(\"script\");e.src=\"https://\"+t.agent,n.parentNode.insertBefore(e,n)}}function o(){\"complete\"===p.readyState&&i()}function i(){c(\"mark\",[\"domContent\",a()+x.offset],null,\"api\")}function a(){return O.exists&&performance.now?Math.round(performance.now()):(s=Math.max((new Date).getTime(),s))-x.offset}var s=(new Date).getTime(),c=t(\"handle\"),f=t(16),u=t(\"ee\"),d=t(15),l=window,p=l.document,h=\"addEventListener\",m=\"attachEvent\",v=l.XMLHttpRequest,w=v&&v.prototype;NREUM.o={ST:setTimeout,SI:l.setImmediate,CT:clearTimeout,XHR:v,REQ:l.Request,EV:l.Event,PR:l.Promise,MO:l.MutationObserver};var y=\"\"+location,g={beacon:\"bam.nr-data.net\",errorBeacon:\"bam.nr-data.net\",agent:\"js-agent.newrelic.com/nr-1099.min.js\"},b=v&&w&&w[h]&&!/CriOS/.test(navigator.userAgent),x=n.exports={offset:s,now:a,origin:y,features:{},xhrWrappable:b,userAgent:d};t(12),p[h]?(p[h](\"DOMContentLoaded\",i,!1),l[h](\"load\",r,!1)):(p[m](\"onreadystatechange\",o),l[m](\"onload\",r)),c(\"mark\",[\"firstbyte\",s],null,\"api\");var E=0,O=t(18)},{}]},{},[\"loader\",2,10,4,3]);</script>\\n\\t<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\\n\\t<title>News | The White House</title>\\n\\n<!-- This site is optimized with the Yoast SEO Premium plugin  - https://yoast.com/wordpress/plugins/seo/ -->\\n<link rel=\"canonical\" href=\"https://www.whitehouse.gov/news/\" />\\n<meta property=\"og:locale\" content=\"en_US\" />\\n<meta property=\"og:type\" content=\"article\" />\\n<meta property=\"og:title\" content=\"News | The White House\" />\\n<meta property=\"og:description\" content=\"Follow the latest updates from the Trump Administration, including briefings and statements, Presidential actions, and news articles.\" />\\n<meta property=\"og:url\" content=\"https://www.whitehouse.gov/news/\" />\\n<meta property=\"og:site_name\" content=\"The White House\" />\\n<meta property=\"article:publisher\" content=\"https://facebook.com/whitehouse\" />\\n<meta property=\"fb:app_id\" content=\"1790466490985150\" />\\n<meta property=\"og:image\" content=\"https://www.whitehouse.gov/wp-content/uploads/2017/12/wh.gov-share-img_03-1024x538.png\" />\\n<meta property=\"og:image:secure_url\" content=\"https://www.whitehouse.gov/wp-content/uploads/2017/12/wh.gov-share-img_03-1024x538.png\" />\\n<meta name=\"twitter:card\" content=\"summary_large_image\" />\\n<meta name=\"twitter:description\" content=\"Follow the latest updates from the Trump Administration, including briefings and statements, Presidential actions, and news articles.\" />\\n<meta name=\"twitter:title\" content=\"News | The White House\" />\\n<meta name=\"twitter:site\" content=\"@whitehouse\" />\\n<meta name=\"twitter:image\" content=\"https://www.whitehouse.gov/wp-content/uploads/2017/12/wh.gov-share-img_03-1024x538.png\" />\\n<meta name=\"twitter:creator\" content=\"@whitehouse\" />\\n<script type=\\'application/ld+json\\'>{\"@context\":\"http:\\\\/\\\\/schema.org\",\"@type\":\"WebSite\",\"@id\":\"#website\",\"url\":\"https:\\\\/\\\\/www.whitehouse.gov\\\\/\",\"name\":\"The White House\",\"alternateName\":\"White House\",\"potentialAction\":{\"@type\":\"SearchAction\",\"target\":\"https:\\\\/\\\\/www.whitehouse.gov\\\\/search\\\\/?s={search_term_string}\",\"query-input\":\"required name=search_term_string\"}}</script>\\n<script type=\\'application/ld+json\\'>{\"@context\":\"http:\\\\/\\\\/schema.org\",\"@type\":\"Organization\",\"url\":\"https:\\\\/\\\\/www.whitehouse.gov\\\\/news\\\\/\",\"sameAs\":[\"https:\\\\/\\\\/facebook.com\\\\/whitehouse\",\"https:\\\\/\\\\/instagram.com\\\\/whitehouse\",\"https:\\\\/\\\\/www.youtube.com\\\\/user\\\\/whitehouse\",\"https:\\\\/\\\\/twitter.com\\\\/whitehouse\"],\"@id\":\"#organization\",\"name\":\"The White House\",\"logo\":\"https:\\\\/\\\\/www.whitehouse.gov\\\\/wp-content\\\\/uploads\\\\/2017\\\\/12\\\\/wh.gov-share-img_03.png\"}</script>\\n<!-- / Yoast SEO Premium plugin. -->\\n\\n<link rel=\\'dns-prefetch\\' href=\\'//s.w.org\\' />\\n<link href=\\'https://fonts.googleapis.com\\' crossorigin rel=\\'preconnect\\' />\\n<link href=\\'https://fonts.gstatic.com\\' crossorigin rel=\\'preconnect\\' />\\n<link rel=\\'stylesheet\\' id=\\'google-fonts-css\\'  href=\\'https://fonts.googleapis.com/css?family=Merriweather:300,300i,400,400i,700,700i,900,900i|Source+Sans+Pro:300,300i,400,400i,600,600i,700,700i,900,900i&#038;subset=latin-ext\\' type=\\'text/css\\' media=\\'all\\' />\\n<link rel=\\'stylesheet\\' id=\\'site-css\\'  href=\\'https://www.whitehouse.gov/wp-content/themes/whitehouse/assets/dist/site.min.css?ver=803d554c\\' type=\\'text/css\\' media=\\'all\\' />\\n<script type=\\'text/javascript\\' src=\\'https://www.whitehouse.gov/wp-content/themes/whitehouse/assets/dist/polyfills.min.js?ver=803d554c\\'></script>\\n<script type=\\'text/javascript\\' src=\\'https://www.whitehouse.gov/wp-content/themes/whitehouse/assets/vendor/picturefill/dist/picturefill.min.js?ver=803d554c\\'></script>\\n<link rel=\"icon\" type=\"image/x-icon\" href=\"/favicon.ico?ver=803d554c\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"32x32\" href=\"/favicon-32x32.png?ver=803d554c\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"16x16\" href=\"/favicon-16x16.png?ver=803d554c\">\\n<link rel=\"apple-touch-icon-precomposed\" sizes=\"180x180\" href=\"/apple-touch-icon-precomposed.png?ver=803d554c\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"256x256\" href=\"/touch-icon-256x256.png?ver=803d554c\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"192x192\" href=\"/touch-icon-192x192.png?ver=803d554c\">\\n<link rel=\"manifest\" href=\"/site.webmanifest?ver=803d554c\">\\n<link rel=\"mask-icon\" href=\"/safari-pinned-tab.svg?ver=803d554c\" color=\"#336799\">\\n<meta name=\"apple-mobile-web-app-title\" content=\"The White House\">\\n<meta name=\"application-name\" content=\"The White House\">\\n<meta name=\"msapplication-TileColor\" content=\"#0c2644\">\\n<meta name=\"theme-color\" content=\"#f5f5f5\">\\n\\t\\t<!-- Global site tag (gtag.js) - Google Analytics -->\\n\\t<script async src=\"https://www.googletagmanager.com/gtag/js?id=UA-12099831-10\"></script>\\n\\t<script>\\n\\t\\twindow.dataLayer = window.dataLayer || [];\\n\\t\\tfunction gtag(){dataLayer.push(arguments);}\\n\\t\\tgtag(\\'js\\', new Date());\\n\\n\\t\\tgtag(\\'config\\', \\'UA-12099831-10\\');\\n\\t</script>\\n</head>\\n<body class=\"page-template-default page-id-655 news single-page\">\\n<a class=\"skip-link\" href=\"#main-content\"><span>Skip to content</span></a>\\n\\n<div class=\"body-overflow\">\\n\\n<header class=\"header header--has-share\">\\n\\t<div class=\"header__container\">\\n\\t\\t<div class=\"header__inner\">\\n\\t\\t\\t<button type=\"button\" class=\"header__hamburger\">\\n\\t\\t\\t\\t<b class=\"svg svg--hamburger\" data-svg-url=\"https://www.whitehouse.gov/wp-content/themes/whitehouse/assets/img/hamburger.svg\" data-img-url=\"https://www.whitehouse.gov/wp-content/themes/whitehouse/assets/img/hamburger.png\" data-icon=\"hamburger\"><svg aria-label=\"Open Menu\" role=\"img\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 14\" height=\"20\" width=\"28\"><path d=\"M.64,12H16.36c.36,0,.64.43.64,1s-.28,1-.64,1H.64C.28,14,0,13.57,0,13s.28-1,.64-1M23.1,2H.9A.94.94,0,0,1,0,1,.94.94,0,0,1,.9,0H23.1A.94.94,0,0,1,24,1a.94.94,0,0,1-.9,1M.9,8A.94.94,0,0,1,0,7,.94.94,0,0,1,.9,6H23.1A.94.94,0,0,1,24,7a.94.94,0,0,1-.9,1Z\" fill=\"#a98860\"/></svg><title>open-menu</title>\\n</b>\\t\\t\\t</button>\\n\\t\\t\\t<div class=\"header__wrap\">\\n\\t\\t\\t\\t<div class=\"header__logo\">\\n\\t\\t\\t\\t\\t<a href=\"https://www.whitehouse.gov\">\\n\\t\\t\\t\\t\\t\\t<img src=\"https://www.whitehouse.gov/wp-content/themes/whitehouse/assets/img/white-house-logo-sm-wh.png\" srcset=\"https://www.whitehouse.gov/wp-content/themes/whitehouse/assets/img/white-house-logo-sm-wh.png 1x, https://www.whitehouse.gov/wp-content/themes/whitehouse/assets/img/white-house-logo-md-wh.png 2x, https://www.whitehouse.gov/wp-content/themes/whitehouse/assets/img/white-house-logo-lg-wh.png 3x\" alt=\"\" class=\"logo--white\" />\\n\\t\\t\\t\\t\\t\\t<img src=\"https://www.whitehouse.gov/wp-content/themes/whitehouse/assets/img/white-house-logo-sm-bl.png\" srcset=\"https://www.whitehouse.gov/wp-content/themes/whitehouse/assets/img/white-house-logo-sm-bl.png 1x, https://www.whitehouse.gov/wp-content/themes/whitehouse/assets/img/white-house-logo-md-bl.png 2x, https://www.whitehouse.gov/wp-content/themes/whitehouse/assets/img/white-house-logo-lg-bl.png 3x\" alt=\"\" class=\"logo--blue\" />\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t<span class=\"visually-hidden\">White House Logo</span>\\n\\t\\t\\t\\t\\t</a>\\n\\t\\t\\t\\t</div>\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<div class=\"header__share\">\\n\\t\\t\\t\\t\\t\\t<a class=\"header__share--copy\" data-clipboard-text=\"https://www.whitehouse.gov/news/?utm_source=link&#038;utm_medium=header\" href=\"https://www.whitehouse.gov/news/?utm_source=link&#038;utm_medium=header\">\\n\\t\\t\\t\\t\\t\\t\\t<b class=\"svg svg--share-link\" data-svg-url=\"https://www.whitehouse.gov/wp-content/themes/whitehouse/assets/img/share-link.svg\" data-icon=\"share-link\"><svg aria-label=\"Copy URL to your clipboard\" role=\"img\" width=\"17\" height=\"15\"><g fill=\"#30669B\"><path d=\"M3.96 9.65c.85.8 2 1.22 3.2 1.22 1.2 0 2.35-.43 3.2-1.22l.94-.88c.35-.33.35-.85 0-1.2-.36-.32-.92-.32-1.28 0l-.94.9c-.5.47-1.2.74-1.92.74-.74 0-1.4-.26-1.93-.74L2.66 6.08c-1.07-1-1.07-2.6 0-3.6 1.08-.98 2.8-.98 3.85 0l.84.77c.35.33.9.33 1.27 0s.37-.85 0-1.2L7.8 1.3c-1.76-1.65-4.63-1.65-6.4 0-1.74 1.64-1.74 4.3 0 5.93l2.58 2.42zM5.7 6.25c-.35.33-.35.85 0 1.2.36.32.92.32 1.28 0l.94-.9c.5-.47 1.2-.74 1.92-.74s1.4.28 1.93.76l2.57 2.4c1.07 1 1.07 2.6 0 3.6-1.08 1-2.8 1-3.87 0l-.83-.77c-.36-.34-.92-.34-1.27 0-.36.32-.36.85 0 1.18l.82.77c.87.8 2.03 1.23 3.2 1.23 1.16 0 2.3-.42 3.2-1.23 1.76-1.65 1.76-4.3 0-5.96l-2.56-2.45c-.85-.8-2-1.22-3.2-1.22-1.2 0-2.35.43-3.2 1.22l-.94.9z\"/></g></svg><title>copy-url-to-your-clipboard</title></b> Share\\n\\t\\t\\t\\t\\t\\t</a>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t\\t\\t\\t<nav class=\"nav nav--primary header__nav\"><ul class=\"nav__menu nav__menu--depth0 menu\"><li class=\"nav__menu-item nav__menu-item--depth0 menu-item menu-item-type-post_type menu-item-object-issue menu-item-692\"><a href=\"https://www.whitehouse.gov/issues/economy-jobs/\" class=\"nav__link\">Economy</a></li>\\n<li class=\"nav__menu-item nav__menu-item--depth0 menu-item menu-item-type-post_type menu-item-object-issue menu-item-693\"><a href=\"https://www.whitehouse.gov/issues/national-security-defense/\" class=\"nav__link\">National Security</a></li>\\n<li class=\"nav__menu-item nav__menu-item--depth0 menu-item menu-item-type-post_type menu-item-object-issue menu-item-16229\"><a href=\"https://www.whitehouse.gov/issues/budget-spending/\" class=\"nav__link\">Budget</a></li>\\n<li class=\"nav__menu-item nav__menu-item--depth0 menu-item menu-item-type-post_type menu-item-object-issue menu-item-16236\"><a href=\"https://www.whitehouse.gov/issues/immigration/\" class=\"nav__link\">Immigration</a></li>\\n<li class=\"nav__menu-item nav__menu-item--depth0 menu-item menu-item-type-post_type menu-item-object-initiative menu-item-38035\"><a href=\"https://www.whitehouse.gov/opioids/\" class=\"nav__link\">The Opioid Crisis</a></li>\\n</ul></nav>\\t\\t\\t</div>\\n\\t\\t\\t<a href=\"https://www.whitehouse.gov/search/\"  class=\"header__search\">\\n\\t\\t\\t\\t<span class=\"visually-hidden\">Search WhiteHouse.gov</span>\\n\\t\\t\\t\\t<b class=\"svg svg--search\" data-svg-url=\"https://www.whitehouse.gov/wp-content/themes/whitehouse/assets/img/search.svg\" data-img-url=\"https://www.whitehouse.gov/wp-content/themes/whitehouse/assets/img/search.png\" data-icon=\"search\"><svg aria-label=\"Open Search\" role=\"img\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 20 20\" height=\"20\" width=\"20\"><title>open-search</title>\\n<path fill=\"#A98860\" d=\"M20,18.7l-5.1-5.1c2.6-3.3,2.4-8.1-0.6-11.2C12.7,0.9,10.6,0,8.4,0C6.1,0,4,0.9,2.5,2.5\\n\\tC0.9,4,0,6.1,0,8.4s0.9,4.3,2.5,5.9c1.6,1.6,3.7,2.5,5.9,2.5c1.9,0,3.8-0.7,5.3-1.8l5.1,5.1L20,18.7z M8.5,15\\n\\tc-1.7,0-3.4-0.7-4.6-1.9C2.7,11.9,2,10.2,2,8.5s0.7-3.4,1.9-4.6C5.1,2.7,6.8,2,8.5,2c1.7,0,3.4,0.7,4.6,1.9c2.5,2.5,2.5,6.7,0,9.2\\n\\tC11.9,14.3,10.2,15,8.5,15z\"/>\\n</svg>\\n</b>\\t\\t\\t</a>\\n\\t\\t</div>\\n\\t</div>\\n</header>\\n<main id=\"main-content\">\\n<div class=\"page-header\">\\n\\t<div class=\"page-header__wrap\">\\n\\t\\t<h1 class=\"page-header__title\">News</h1>\\n\\t\\t<span class=\"divider\"><hr><hr></span>\\n\\t</div>\\n</div>\\n\\n<div class=\"page-results\">\\n\\t<div class=\"page-results__wrap\">\\n\\t\\t<nav class=\"nav nav--news\">\\n\\t<ul class=\"nav__menu\">\\n\\t\\t<li class=\"nav__menu-item nav__menu-item--active\"><a href=\"https://www.whitehouse.gov/news/\" class=\"nav__link\">All News</a></li><li class=\"nav__menu-item\"><a href=\"https://www.whitehouse.gov/articles/\" class=\"nav__link\">Articles</a></li><li class=\"nav__menu-item\"><a href=\"https://www.whitehouse.gov/presidential-actions/\" class=\"nav__link\">Presidential Actions</a></li><li class=\"nav__menu-item\"><a href=\"https://www.whitehouse.gov/briefings-statements/\" class=\"nav__link\">Briefings &amp; Statements</a></li>\\t</ul>\\n</nav>\\n\\n\\t<ul class=\"filters__toggle-facets\">\\n\\t\\t<li>\\n\\t\\t\\t<button type=\"button\" class=\"filters__toggle-facets-issue\">\\n\\t\\t\\t\\t<b class=\"svg svg--filter\" data-svg-url=\"https://www.whitehouse.gov/wp-content/themes/whitehouse/assets/img/filter.svg\" data-icon=\"filter\"><svg aria-label=\"Filter by Issue\" role=\"img\" xmlns=\"http://www.w3.org/2000/svg\" width=\"18\" height=\"14\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs><path id=\"a\" d=\"M553.63 457c.2 0 .37.43.37 1 0 .57-.16 1-.37 1h-9.25c-.22 0-.38-.43-.38-1 0-.57.16-1 .38-1zm-8.96-10c-.38 0-.67-.43-.67-1 0-.57.3-1 .67-1h16.66c.38 0 .67.43.67 1 0 .57-.3 1-.67 1zm0 6c-.38 0-.67-.43-.67-1 0-.57.3-1 .67-1h16.66c.38 0 .67.43.67 1 0 .57-.3 1-.67 1z\"/></defs><use fill=\"#a98860\" xlink:href=\"#a\" transform=\"translate(-544 -445)\"/></svg><title>filter-by-issue</title></b> Filter by Issue\\n\\t\\t\\t</button>\\n\\t\\t</li>\\n\\t</ul>\\n\\n\\t<nav class=\"filters__facets\">\\n\\t\\t<div class=\"wrap\">\\n\\t\\t\\t<p class=\"filters__close\"><button type=\"button\"><b class=\"svg svg--close\" data-svg-url=\"https://www.whitehouse.gov/wp-content/themes/whitehouse/assets/img/close.svg\" data-icon=\"close\"><svg aria-label=\"Close Filter by Issue\" role=\"img\" xmlns=\"http://www.w3.org/2000/svg\" width=\"19\" height=\"18\"><path fill=\"#A98860\" d=\"M17.78 14.9l-6.73-6.43 6.5-6.64V1.8c.37-.4.36-1.04-.04-1.4-.2-.2-.47-.3-.76-.3-.27 0-.5.13-.7.32L9.56 7.07 2.64.42c-.2-.2-.48-.3-.77-.3-.27 0-.52.14-.7.33C1 .65.87.92.9 1.2c0 .26.13.5.32.68l6.9 6.64-6.7 6.88c-.2.2-.3.47-.3.76 0 .27.14.5.33.7.15.1.38.27.73.27.37 0 .6-.18.74-.3l6.7-6.88 6.72 6.42c.14.15.37.28.75.28.36 0 .58-.18.73-.3.2-.2.3-.48.3-.76-.03-.28-.14-.52-.35-.7\"/></svg><title>close-filter-by-issue</title></b>Filter by Issue</button></p>\\n\\t\\t\\t<ul class=\"filters__facets-list\">\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<li class=\"filters__facets-list-item filters__facets-list-item--active\"><span>All Issues</span></li>\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t<li class=\"filters__facets-list-item\"><a href=\"https://www.whitehouse.gov/news/?issue_filter=budget-spending\">Budget &amp; Spending</a></li><li class=\"filters__facets-list-item\"><a href=\"https://www.whitehouse.gov/news/?issue_filter=economy-jobs\">Economy &amp; Jobs</a></li><li class=\"filters__facets-list-item\"><a href=\"https://www.whitehouse.gov/news/?issue_filter=education\">Education</a></li><li class=\"filters__facets-list-item\"><a href=\"https://www.whitehouse.gov/news/?issue_filter=energy-environment\">Energy &amp; Environment</a></li><li class=\"filters__facets-list-item\"><a href=\"https://www.whitehouse.gov/news/?issue_filter=foreign-policy\">Foreign Policy</a></li><li class=\"filters__facets-list-item\"><a href=\"https://www.whitehouse.gov/news/?issue_filter=healthcare\">Healthcare</a></li><li class=\"filters__facets-list-item\"><a href=\"https://www.whitehouse.gov/news/?issue_filter=immigration\">Immigration</a></li><li class=\"filters__facets-list-item\"><a href=\"https://www.whitehouse.gov/news/?issue_filter=infrastructure-technology\">Infrastructure &amp; Technology</a></li><li class=\"filters__facets-list-item\"><a href=\"https://www.whitehouse.gov/news/?issue_filter=land-agriculture\">Land &amp; Agriculture</a></li><li class=\"filters__facets-list-item\"><a href=\"https://www.whitehouse.gov/news/?issue_filter=law-justice\">Law &amp; Justice</a></li><li class=\"filters__facets-list-item\"><a href=\"https://www.whitehouse.gov/news/?issue_filter=national-security-defense\">National Security &amp; Defense</a></li><li class=\"filters__facets-list-item\"><a href=\"https://www.whitehouse.gov/news/?issue_filter=social-programs\">Social Programs</a></li><li class=\"filters__facets-list-item\"><a href=\"https://www.whitehouse.gov/news/?issue_filter=veterans\">Veterans</a></li>\\t\\t\\t</ul>\\n\\t\\t</div>\\n\\t</nav>\\n\\n\\t\\n\\t\\t\\t<article class=\"briefing-statement briefing-statement--results\">\\n\\t<div class=\"briefing-statement__content\">\\n\\t\\t\\t\\t\\t<p class=\"briefing-statement__type\">\\n\\t\\t\\t\\tNews Clips\\t\\t\\t</p>\\n\\t\\t\\n\\t\\t<h2 class=\"briefing-statement__title\"><a href=\"https://www.whitehouse.gov/briefings-statements/administrator-mcmahon-small-business-saturday-shows-big-economic-benefits-shopping-small/\">Administrator McMahon: \\xe2\\x80\\x9cSmall Business Saturday Shows the Big Economic Benefits of Shopping Small\\xe2\\x80\\x9d</a></h2>\\n\\n\\t\\t<div class=\"meta meta--left\">\\n\\t\\t\\t\\t\\t\\t\\t<div class=\"meta__issue\">\\n\\t\\t\\t\\t\\t<p class=\"issue-flag issue-flag--left\">\\n\\t\\t\\t<a href=\"https://www.whitehouse.gov/issues/economy-jobs/\">\\n\\t\\t\\tEconomy &#038; Jobs\\t\\t</a>\\n\\t\\t</p>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t\\t<p class=\"meta__date\">\\n\\t\\t\\t\\t<time>Nov 23, 2018</time>\\n\\t\\t\\t</p>\\n\\t\\t</div>\\n\\t</div>\\n</article>\\n<article class=\"briefing-statement briefing-statement--results\">\\n\\t<div class=\"briefing-statement__content\">\\n\\t\\t\\t\\t\\t<p class=\"briefing-statement__type\">\\n\\t\\t\\t\\tRemarks\\t\\t\\t</p>\\n\\t\\t\\n\\t\\t<h2 class=\"briefing-statement__title\"><a href=\"https://www.whitehouse.gov/briefings-statements/remarks-president-trump-thanksgiving-teleconference-members-military/\">Remarks by President Trump in Thanksgiving Teleconference with Members of the Military</a></h2>\\n\\n\\t\\t<div class=\"meta meta--left\">\\n\\t\\t\\t\\t\\t\\t\\t<div class=\"meta__issue\">\\n\\t\\t\\t\\t\\t<p class=\"issue-flag issue-flag--left\">\\n\\t\\t\\t<a href=\"https://www.whitehouse.gov/issues/national-security-defense/\">\\n\\t\\t\\tNational Security &#038; Defense\\t\\t</a>\\n\\t\\t</p>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t\\t<p class=\"meta__date\">\\n\\t\\t\\t\\t<time>Nov 23, 2018</time>\\n\\t\\t\\t</p>\\n\\t\\t</div>\\n\\t</div>\\n</article>\\n<article class=\"briefing-statement briefing-statement--results\">\\n\\t<div class=\"briefing-statement__content\">\\n\\t\\t\\t\\t\\t<p class=\"briefing-statement__type\">\\n\\t\\t\\t\\tRemarks\\t\\t\\t</p>\\n\\t\\t\\n\\t\\t<h2 class=\"briefing-statement__title\"><a href=\"https://www.whitehouse.gov/briefings-statements/remarks-president-trump-members-coast-guard/\">Remarks by President Trump to Members of the Coast Guard</a></h2>\\n\\n\\t\\t<div class=\"meta meta--left\">\\n\\t\\t\\t\\t\\t\\t\\t<div class=\"meta__issue\">\\n\\t\\t\\t\\t\\t<p class=\"issue-flag issue-flag--left\">\\n\\t\\t\\t<a href=\"https://www.whitehouse.gov/issues/national-security-defense/\">\\n\\t\\t\\tNational Security &#038; Defense\\t\\t</a>\\n\\t\\t</p>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t\\t<p class=\"meta__date\">\\n\\t\\t\\t\\t<time>Nov 22, 2018</time>\\n\\t\\t\\t</p>\\n\\t\\t</div>\\n\\t</div>\\n</article>\\n<article class=\"presidential-action presidential-action--results\">\\n\\t<div class=\"presidential-action__content\">\\n\\t\\t\\t\\t\\t<p class=\"presidential-action__type\">\\n\\t\\t\\t\\tNominations &amp; Appointments\\t\\t\\t</p>\\n\\t\\t\\n\\t\\t<h2 class=\"presidential-action__title\"><a href=\"https://www.whitehouse.gov/presidential-actions/president-donald-j-trump-announces-intent-nominate-individuals-key-administration-posts-3/\">President Donald J. Trump Announces Intent to Nominate and Appoint Individuals to Key Administration Posts</a></h2>\\n\\n\\t\\t<div class=\"meta meta--left\">\\n\\t\\t\\t\\t\\t\\t<p class=\"meta__date\">\\n\\t\\t\\t\\t<time>Nov 21, 2018</time>\\n\\t\\t\\t</p>\\n\\t\\t</div>\\n\\t</div>\\n</article>\\n<article class=\"briefing-statement briefing-statement--results\">\\n\\t<div class=\"briefing-statement__content\">\\n\\t\\t\\t\\t\\t<p class=\"briefing-statement__type\">\\n\\t\\t\\t\\tRemarks\\t\\t\\t</p>\\n\\t\\t\\n\\t\\t<h2 class=\"briefing-statement__title\"><a href=\"https://www.whitehouse.gov/briefings-statements/remarks-president-trump-national-thanksgiving-turkey-pardoning-ceremony-2/\">Remarks by President Trump at the National Thanksgiving Turkey Pardoning Ceremony</a></h2>\\n\\n\\t\\t<div class=\"meta meta--left\">\\n\\t\\t\\t\\t\\t\\t<p class=\"meta__date\">\\n\\t\\t\\t\\t<time>Nov 20, 2018</time>\\n\\t\\t\\t</p>\\n\\t\\t</div>\\n\\t</div>\\n</article>\\n<article class=\"briefing-statement briefing-statement--results\">\\n\\t<div class=\"briefing-statement__content\">\\n\\t\\t\\t\\t\\t<p class=\"briefing-statement__type\">\\n\\t\\t\\t\\tStatements &amp; Releases\\t\\t\\t</p>\\n\\t\\t\\n\\t\\t<h2 class=\"briefing-statement__title\"><a href=\"https://www.whitehouse.gov/briefings-statements/president-donald-j-trump-signed-h-r-2615-law/\">President Donald J. Trump Signed H.R. 2615 into Law</a></h2>\\n\\n\\t\\t<div class=\"meta meta--left\">\\n\\t\\t\\t\\t\\t\\t\\t<div class=\"meta__issue\">\\n\\t\\t\\t\\t\\t<p class=\"issue-flag issue-flag--left\">\\n\\t\\t\\t<a href=\"https://www.whitehouse.gov/issues/land-agriculture/\">\\n\\t\\t\\tLand &#038; Agriculture\\t\\t</a>\\n\\t\\t</p>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t\\t<p class=\"meta__date\">\\n\\t\\t\\t\\t<time>Nov 20, 2018</time>\\n\\t\\t\\t</p>\\n\\t\\t</div>\\n\\t</div>\\n</article>\\n<article class=\"briefing-statement briefing-statement--results\">\\n\\t<div class=\"briefing-statement__content\">\\n\\t\\t\\t\\t\\t<p class=\"briefing-statement__type\">\\n\\t\\t\\t\\tRemarks\\t\\t\\t</p>\\n\\t\\t\\n\\t\\t<h2 class=\"briefing-statement__title\"><a href=\"https://www.whitehouse.gov/briefings-statements/remarks-president-trump-marine-one-departure-25/\">Remarks by President Trump Before Marine One Departure</a></h2>\\n\\n\\t\\t<div class=\"meta meta--left\">\\n\\t\\t\\t\\t\\t\\t<p class=\"meta__date\">\\n\\t\\t\\t\\t<time>Nov 20, 2018</time>\\n\\t\\t\\t</p>\\n\\t\\t</div>\\n\\t</div>\\n</article>\\n<article class=\"briefing-statement briefing-statement--results\">\\n\\t<div class=\"briefing-statement__content\">\\n\\t\\t\\t\\t\\t<p class=\"briefing-statement__type\">\\n\\t\\t\\t\\tStatements &amp; Releases\\t\\t\\t</p>\\n\\t\\t\\n\\t\\t<h2 class=\"briefing-statement__title\"><a href=\"https://www.whitehouse.gov/briefings-statements/statement-press-secretary-41/\">Statement from the Press Secretary</a></h2>\\n\\n\\t\\t<div class=\"meta meta--left\">\\n\\t\\t\\t\\t\\t\\t\\t<div class=\"meta__issue\">\\n\\t\\t\\t\\t\\t<p class=\"issue-flag issue-flag--left\">\\n\\t\\t\\t<a href=\"https://www.whitehouse.gov/issues/immigration/\">\\n\\t\\t\\tImmigration\\t\\t</a>\\n\\t\\t</p>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t\\t<p class=\"meta__date\">\\n\\t\\t\\t\\t<time>Nov 20, 2018</time>\\n\\t\\t\\t</p>\\n\\t\\t</div>\\n\\t</div>\\n</article>\\n<article class=\"briefing-statement briefing-statement--results\">\\n\\t<div class=\"briefing-statement__content\">\\n\\t\\t\\t\\t\\t<p class=\"briefing-statement__type\">\\n\\t\\t\\t\\tFact Sheets\\t\\t\\t</p>\\n\\t\\t\\n\\t\\t<h2 class=\"briefing-statement__title\"><a href=\"https://www.whitehouse.gov/briefings-statements/president-donald-j-trump-urges-congressional-action-improve-forest-management-help-prevent-wildfires/\">President Donald J. Trump Urges Congressional Action to Improve Forest Management and Help Prevent Wildfires</a></h2>\\n\\n\\t\\t<div class=\"meta meta--left\">\\n\\t\\t\\t\\t\\t\\t\\t<div class=\"meta__issue\">\\n\\t\\t\\t\\t\\t<p class=\"issue-flag issue-flag--left\">\\n\\t\\t\\t<a href=\"https://www.whitehouse.gov/issues/land-agriculture/\">\\n\\t\\t\\tLand &#038; Agriculture\\t\\t</a>\\n\\t\\t</p>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t\\t<p class=\"meta__date\">\\n\\t\\t\\t\\t<time>Nov 20, 2018</time>\\n\\t\\t\\t</p>\\n\\t\\t</div>\\n\\t</div>\\n</article>\\n<article class=\"briefing-statement briefing-statement--results\">\\n\\t<div class=\"briefing-statement__content\">\\n\\t\\t\\t\\t\\t<p class=\"briefing-statement__type\">\\n\\t\\t\\t\\tStatements &amp; Releases\\t\\t\\t</p>\\n\\t\\t\\n\\t\\t<h2 class=\"briefing-statement__title\"><a href=\"https://www.whitehouse.gov/briefings-statements/statement-president-donald-j-trump-standing-saudi-arabia/\">Statement from President Donald J. Trump\\xc2\\xa0on Standing with Saudi Arabia</a></h2>\\n\\n\\t\\t<div class=\"meta meta--left\">\\n\\t\\t\\t\\t\\t\\t\\t<div class=\"meta__issue\">\\n\\t\\t\\t\\t\\t<p class=\"issue-flag issue-flag--left\">\\n\\t\\t\\t<a href=\"https://www.whitehouse.gov/issues/foreign-policy/\">\\n\\t\\t\\tForeign Policy\\t\\t</a>\\n\\t\\t</p>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t\\t<p class=\"meta__date\">\\n\\t\\t\\t\\t<time>Nov 20, 2018</time>\\n\\t\\t\\t</p>\\n\\t\\t</div>\\n\\t</div>\\n</article>\\n\\t<!-- no pagination -->\\n\\t\\t\\t</div>\\n</div>\\n\\n</main><!-- end main -->\\n\\n<footer class=\"footer\">\\n\\t<div class=\"footer__wrap\">\\n\\t\\t<div class=\"footer__logo\">\\n\\t\\t\\t<a href=\"https://www.whitehouse.gov\">\\n\\t\\t\\t\\t<img class=\"footer__img\" src=\"https://www.whitehouse.gov/wp-content/themes/whitehouse/assets/img/white-house-logo-footer-sm.png\" srcset=\"https://www.whitehouse.gov/wp-content/themes/whitehouse/assets/img/white-house-logo-footer-sm.png 1x, https://www.whitehouse.gov/wp-content/themes/whitehouse/assets/img/white-house-logo-footer-md.png 2x, https://www.whitehouse.gov/wp-content/themes/whitehouse/assets/img/white-house-logo-footer-lg.png 3x\" alt=\"White House Logo\">\\n\\t\\t\\t</a>\\n\\t\\t</div>\\n\\t\\t<div class=\"footer__site-title\">The White House</div>\\n\\t\\t<span class=\"footer__divider divider\"><hr><hr></span>\\n\\t\\t<div class=\"footer__nav\">\\n\\t\\t\\t<nav class=\"nav nav--footer\"><ul class=\"nav__menu nav__menu--depth0 menu\"><li class=\"nav__menu-item nav__menu-item--depth0 menu-item menu-item-type-post_type menu-item-object-page menu-item-17230\"><a href=\"https://www.whitehouse.gov/live/\" class=\"nav__link\">Live</a></li>\\n<li class=\"nav__menu-item nav__menu-item--depth0 menu-item menu-item-type-custom menu-item-object-custom menu-item-8288\"><a href=\"https://apply.whitehouse.gov/\" class=\"nav__link\">Jobs</a></li>\\n<li class=\"nav__menu-item nav__menu-item--depth0 menu-item menu-item-type-post_type menu-item-object-page menu-item-8093\"><a href=\"https://www.whitehouse.gov/get-involved/\" class=\"nav__link\">Get Involved</a></li>\\n<li class=\"nav__menu-item nav__menu-item--depth0 menu-item menu-item-type-post_type menu-item-object-page menu-item-8092\"><a href=\"https://www.whitehouse.gov/copyright/\" class=\"nav__link\">Copyright Policy</a></li>\\n<li class=\"nav__menu-item nav__menu-item--depth0 menu-item menu-item-type-post_type menu-item-object-page menu-item-8095\"><a href=\"https://www.whitehouse.gov/privacy-policy/\" class=\"nav__link\">Privacy Policy</a></li>\\n</ul></nav>\\t\\t</div>\\n\\t</div>\\n\\t<div class=\"footer__social\">\\n\\t\\t<nav class=\"nav nav--social-links-menu\"><ul class=\"nav__menu nav__menu--depth0 menu\"><li class=\"nav__menu-item nav__menu-item--depth0 footer__social-link__twitter menu-item menu-item-type-custom menu-item-object-custom menu-item-21\"><a target=\"_blank\" href=\"https://twitter.com/whitehouse\" class=\"nav__link\"><span>Twitter</span></a></li>\\n<li class=\"nav__menu-item nav__menu-item--depth0 footer__social-link__facebook menu-item menu-item-type-custom menu-item-object-custom menu-item-20\"><a target=\"_blank\" href=\"https://www.facebook.com/WhiteHouse/\" class=\"nav__link\"><span>Facebook</span></a></li>\\n<li class=\"nav__menu-item nav__menu-item--depth0 footer__social-link__instagram menu-item menu-item-type-custom menu-item-object-custom menu-item-22\"><a target=\"_blank\" href=\"https://www.instagram.com/whitehouse/\" class=\"nav__link\"><span>Instagram</span></a></li>\\n<li class=\"nav__menu-item nav__menu-item--depth0 footer__social-link__email menu-item menu-item-type-custom menu-item-object-custom menu-item-23\"><a href=\"/contact/\" class=\"nav__link\"><span>Contact</span></a></li>\\n</ul></nav>\\t</div>\\n</footer>\\n</div><!-- end .body-overflow -->\\n\\n<section id=\"wp-footer\">\\n\\t<script type=\\'text/javascript\\' src=\\'https://www.whitehouse.gov/wp-content/themes/whitehouse/assets/vendor/jquery/dist/jquery.min.js?ver=803d554c\\'></script>\\n<script type=\\'text/javascript\\' src=\\'https://www.whitehouse.gov/wp-content/themes/whitehouse/assets/vendor/clipboard/dist/clipboard.min.js?ver=803d554c\\'></script>\\n<script type=\\'text/javascript\\' src=\\'https://www.whitehouse.gov/wp-content/themes/whitehouse/assets/vendor/bows/dist/bows.min.js?ver=803d554c\\'></script>\\n<script type=\\'text/javascript\\' src=\\'https://www.whitehouse.gov/wp-content/themes/whitehouse/assets/dist/site.min.js?ver=803d554c\\'></script>\\n<script type=\\'text/javascript\\'>\\nwindow.site = new Site();\\n</script>\\n</section><div class=\"popover\" role=\"dialog\">\\n\\t<div class=\"popover__bar\">\\n\\t\\t<button type=\"button\" class=\"popover__hamburger-toggle\">\\n\\t\\t\\t<img class=\"popover__hamburger-toggle-close\" src=\"https://www.whitehouse.gov/wp-content/themes/whitehouse/assets/img/close.svg\" alt=\"Close Menu\" width=\"20\" height=\"20\" />\\n\\t\\t\\t<img class=\"popover__hamburger-toggle-open\" src=\"https://www.whitehouse.gov/wp-content/themes/whitehouse/assets/img/hamburger.svg\" alt=\"Open Menu\" width=\"28\" height=\"20\" />\\n\\t\\t</button>\\n\\n\\t\\t<button type=\"button\" class=\"popover__search-toggle\">\\n\\t\\t\\t<img class=\"popover__search-toggle-close\" src=\"https://www.whitehouse.gov/wp-content/themes/whitehouse/assets/img/close.svg\" alt=\"Close Search\" width=\"20\" height=\"20\" />\\n\\t\\t\\t<img class=\"popover__search-toggle-open\" src=\"https://www.whitehouse.gov/wp-content/themes/whitehouse/assets/img/search.svg\" alt=\"Open Search\" width=\"20\" height=\"20\" />\\n\\t\\t</button>\\n\\t</div>\\n\\n\\t<div class=\"popover__content\">\\n\\t\\t<div class=\"popover__hamburger\">\\n\\t\\t\\t<div class=\"popover__nav\">\\n\\t\\t\\t\\t<nav class=\"nav nav--popover\"><ul class=\"nav__menu nav__menu--depth0 menu\"><li class=\"nav__menu-item nav__menu-item--depth0 nav__menu-item--has-submenu nav__menu-item--active nav__menu-item--active-ancestor menu-item menu-item-type-post_type menu-item-object-page current-menu-item page_item page-item-655 current_page_item current-menu-ancestor current-menu-parent current_page_parent current_page_ancestor menu-item-has-children nav__menu-item--has-menu-4 menu-item-658\"><a href=\"https://www.whitehouse.gov/news/\" class=\"nav__link\">News</a>\\n<ul class=\"nav__menu nav__menu--depth1 sub-menu\">\\n\\t<li class=\"nav__menu-item nav__menu-item--depth1 menu-item menu-item-type-post_type_archive menu-item-object-briefing-statement menu-item-105680\"><a href=\"https://www.whitehouse.gov/briefings-statements/\" class=\"nav__link\">Briefings &#038; Statements</a></li>\\n\\t<li class=\"nav__menu-item nav__menu-item--depth1 menu-item menu-item-type-post_type_archive menu-item-object-presidential-action menu-item-105687\"><a href=\"https://www.whitehouse.gov/presidential-actions/\" class=\"nav__link\">Presidential Actions</a></li>\\n\\t<li class=\"nav__menu-item nav__menu-item--depth1 menu-item menu-item-type-post_type menu-item-object-page menu-item-105701\"><a href=\"https://www.whitehouse.gov/articles/\" class=\"nav__link\">Articles</a></li>\\n\\t<li class=\"nav__menu-item nav__menu-item--depth1 nav__menu-item--active menu-item menu-item-type-post_type menu-item-object-page current-menu-item page_item page-item-655 current_page_item menu-item-105694\"><a href=\"https://www.whitehouse.gov/news/\" class=\"nav__link\">All News</a></li>\\n</ul>\\n</li>\\n<li class=\"nav__menu-item nav__menu-item--depth0 nav__menu-item--has-submenu menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children nav__menu-item--has-menu-6 menu-item-659\"><a href=\"https://www.whitehouse.gov/issues/\" class=\"nav__link\">Issues</a>\\n<ul class=\"nav__menu nav__menu--depth1 sub-menu\">\\n\\t<li class=\"nav__menu-item nav__menu-item--depth1 menu-item menu-item-type-post_type menu-item-object-issue menu-item-105708\"><a href=\"https://www.whitehouse.gov/issues/education/\" class=\"nav__link\">Education</a></li>\\n\\t<li class=\"nav__menu-item nav__menu-item--depth1 menu-item menu-item-type-post_type menu-item-object-issue menu-item-105715\"><a href=\"https://www.whitehouse.gov/issues/national-security-defense/\" class=\"nav__link\">National Security &#038; Defense</a></li>\\n\\t<li class=\"nav__menu-item nav__menu-item--depth1 menu-item menu-item-type-post_type menu-item-object-issue menu-item-105729\"><a href=\"https://www.whitehouse.gov/issues/budget-spending/\" class=\"nav__link\">Budget &#038; Spending</a></li>\\n\\t<li class=\"nav__menu-item nav__menu-item--depth1 menu-item menu-item-type-post_type menu-item-object-issue menu-item-105722\"><a href=\"https://www.whitehouse.gov/issues/immigration/\" class=\"nav__link\">Immigration</a></li>\\n\\t<li class=\"nav__menu-item nav__menu-item--depth1 menu-item menu-item-type-post_type menu-item-object-issue menu-item-105736\"><a href=\"https://www.whitehouse.gov/issues/healthcare/\" class=\"nav__link\">Healthcare</a></li>\\n\\t<li class=\"nav__menu-item nav__menu-item--depth1 menu-item menu-item-type-post_type menu-item-object-page menu-item-105743\"><a href=\"https://www.whitehouse.gov/issues/\" class=\"nav__link\">All Issues</a></li>\\n</ul>\\n</li>\\n<li class=\"nav__menu-item nav__menu-item--depth0 nav__menu-item--has-submenu menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children nav__menu-item--has-menu-6 menu-item-795\"><a href=\"https://www.whitehouse.gov/the-trump-administration/\" class=\"nav__link\">The Administration</a>\\n<ul class=\"nav__menu nav__menu--depth1 sub-menu\">\\n\\t<li class=\"nav__menu-item nav__menu-item--depth1 menu-item menu-item-type-post_type menu-item-object-authors menu-item-105757\"><a href=\"https://www.whitehouse.gov/people/donald-j-trump/\" class=\"nav__link\">President Donald J. Trump</a></li>\\n\\t<li class=\"nav__menu-item nav__menu-item--depth1 menu-item menu-item-type-post_type menu-item-object-authors menu-item-105764\"><a href=\"https://www.whitehouse.gov/people/mike-pence/\" class=\"nav__link\">Vice President Michael R. Pence</a></li>\\n\\t<li class=\"nav__menu-item nav__menu-item--depth1 menu-item menu-item-type-post_type menu-item-object-authors menu-item-105750\"><a href=\"https://www.whitehouse.gov/people/melania-trump/\" class=\"nav__link\">First Lady Melania Trump</a></li>\\n\\t<li class=\"nav__menu-item nav__menu-item--depth1 menu-item menu-item-type-post_type menu-item-object-authors menu-item-105771\"><a href=\"https://www.whitehouse.gov/people/karen-pence/\" class=\"nav__link\">Mrs. Karen Pence</a></li>\\n\\t<li class=\"nav__menu-item nav__menu-item--depth1 menu-item menu-item-type-post_type menu-item-object-page menu-item-105778\"><a href=\"https://www.whitehouse.gov/the-trump-administration/the-cabinet/\" class=\"nav__link\">The Cabinet</a></li>\\n\\t<li class=\"nav__menu-item nav__menu-item--depth1 menu-item menu-item-type-post_type menu-item-object-page menu-item-105785\"><a href=\"https://www.whitehouse.gov/disclosures/\" class=\"nav__link\">Disclosures</a></li>\\n</ul>\\n</li>\\n<li class=\"nav__menu-item nav__menu-item--depth0 nav__menu-item--has-submenu menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children nav__menu-item--has-menu-6 menu-item-105792\"><a href=\"#\" class=\"nav__link\">Executive Offices</a>\\n<ul class=\"nav__menu nav__menu--depth1 sub-menu\">\\n\\t<li class=\"nav__menu-item nav__menu-item--depth1 menu-item menu-item-type-post_type menu-item-object-eop-component menu-item-105806\"><a href=\"https://www.whitehouse.gov/cea/\" class=\"nav__link\">Council of Economic Advisers</a></li>\\n\\t<li class=\"nav__menu-item nav__menu-item--depth1 menu-item menu-item-type-post_type menu-item-object-eop-component menu-item-105799\"><a href=\"https://www.whitehouse.gov/ceq/\" class=\"nav__link\">Council on Environmental Quality</a></li>\\n\\t<li class=\"nav__menu-item nav__menu-item--depth1 menu-item menu-item-type-post_type menu-item-object-eop-component menu-item-105813\"><a href=\"https://www.whitehouse.gov/nsc/\" class=\"nav__link\">National Security Council</a></li>\\n\\t<li class=\"nav__menu-item nav__menu-item--depth1 menu-item menu-item-type-post_type menu-item-object-eop-component menu-item-106142\"><a href=\"https://www.whitehouse.gov/omb/\" class=\"nav__link\">Office of Management and Budget</a></li>\\n\\t<li class=\"nav__menu-item nav__menu-item--depth1 menu-item menu-item-type-post_type menu-item-object-eop-component menu-item-105827\"><a href=\"https://www.whitehouse.gov/ondcp/\" class=\"nav__link\">Office of National Drug Control Policy</a></li>\\n\\t<li class=\"nav__menu-item nav__menu-item--depth1 menu-item menu-item-type-post_type menu-item-object-eop-component menu-item-105834\"><a href=\"https://www.whitehouse.gov/ostp/\" class=\"nav__link\">Office of Science and Technology Policy</a></li>\\n</ul>\\n</li>\\n<li class=\"nav__menu-item nav__menu-item--depth0 nav__menu-item--has-submenu menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children nav__menu-item--has-menu-5 menu-item-1548\"><a href=\"https://www.whitehouse.gov/about-the-white-house/\" class=\"nav__link\">About The White House</a>\\n<ul class=\"nav__menu nav__menu--depth1 sub-menu\">\\n\\t<li class=\"nav__menu-item nav__menu-item--depth1 menu-item menu-item-type-post_type menu-item-object-page menu-item-105848\"><a href=\"https://www.whitehouse.gov/about-the-white-house/tours-events/\" class=\"nav__link\">Tours &#038; Events</a></li>\\n\\t<li class=\"nav__menu-item nav__menu-item--depth1 menu-item menu-item-type-post_type menu-item-object-page menu-item-105855\"><a href=\"https://www.whitehouse.gov/about-the-white-house/presidents/\" class=\"nav__link\">Past Presidents</a></li>\\n\\t<li class=\"nav__menu-item nav__menu-item--depth1 menu-item menu-item-type-post_type menu-item-object-page menu-item-105862\"><a href=\"https://www.whitehouse.gov/about-the-white-house/first-ladies/\" class=\"nav__link\">Past First Ladies</a></li>\\n\\t<li class=\"nav__menu-item nav__menu-item--depth1 nav__menu-item--has-submenu menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children nav__menu-item--has-menu-5 menu-item-105995\"><a href=\"https://www.whitehouse.gov/about-the-white-house/the-grounds/\" class=\"nav__link\">The Grounds</a></li>\\n\\t<li class=\"nav__menu-item nav__menu-item--depth1 nav__menu-item--has-submenu menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children nav__menu-item--has-menu-7 menu-item-106079\"><a href=\"https://www.whitehouse.gov/about-the-white-house/our-government/\" class=\"nav__link\">Our Government</a></li>\\n</ul>\\n</li>\\n<li class=\"nav__menu-item nav__menu-item--depth0 nav__menu-item--has-submenu menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children nav__menu-item--has-menu-3 menu-item-5738\"><a href=\"https://www.whitehouse.gov/get-involved/\" class=\"nav__link\">Get Involved</a>\\n<ul class=\"nav__menu nav__menu--depth1 sub-menu\">\\n\\t<li class=\"nav__menu-item nav__menu-item--depth1 menu-item menu-item-type-post_type menu-item-object-page menu-item-105869\"><a href=\"https://www.whitehouse.gov/get-involved/write-or-call/\" class=\"nav__link\">Write or Call the White House</a></li>\\n\\t<li class=\"nav__menu-item nav__menu-item--depth1 nav__menu-item--has-submenu menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children nav__menu-item--has-menu-3 menu-item-105876\"><a href=\"https://www.whitehouse.gov/get-involved/internships/\" class=\"nav__link\">White House Internship Program</a></li>\\n\\t<li class=\"nav__menu-item nav__menu-item--depth1 nav__menu-item--has-submenu menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children nav__menu-item--has-menu-4 menu-item-105904\"><a href=\"https://www.whitehouse.gov/get-involved/fellows/\" class=\"nav__link\">White House Fellows</a></li>\\n</ul>\\n</li>\\n<li class=\"nav__menu-item nav__menu-item--depth0 menu-item menu-item-type-post_type menu-item-object-page menu-item-105988\"><a href=\"https://www.whitehouse.gov/contact/\" class=\"nav__link\">Contact</a></li>\\n<li class=\"nav__menu-item nav__menu-item--depth0 menu-item menu-item-type-post_type menu-item-object-page nav__menu-item--live menu-item-105974\"><a href=\"https://www.whitehouse.gov/live/\" class=\"nav__link\"><span class=\"nav__menu-item-label\">Live</span></a></li>\\n</ul></nav>\\t\\t\\t</div>\\n\\t\\t\\t<div class=\"popover__seal\">\\n\\t\\t\\t\\t<img height=\"435\" width=\"435\" src=\"https://www.whitehouse.gov/wp-content/themes/whitehouse/assets/img/presidential-crest-gray.svg\" alt=\"Presidential Crest\" />\\n\\t\\t\\t</div>\\n\\t\\t</div>\\n\\n\\t\\t<div class=\"popover__search\">\\n\\t\\t\\t<form class=\"popover__search-form\" action=\"https://www.whitehouse.gov/search/\">\\n\\t\\t\\t\\t<label class=\"visually-hidden\" for=\"popover__search-input\">\\n\\t\\t\\t\\t\\tType Your Search\\n\\t\\t\\t\\t</label>\\n\\t\\t\\t\\t<input type=\"text\" id=\"popover__search-input\" class=\"popover__search-input\" name=\"s\" placeholder=\"Type Your Search...\" autocomplete=\"off\">\\n\\t\\t\\t\\t<button type=\"submit\" class=\"popover__search-submit\"><span>Press enter to search</span></button>\\n\\t\\t\\t</form>\\n\\t\\t</div>\\n\\t</div>\\n</div>\\n\\n<script type=\"text/javascript\">window.NREUM||(NREUM={});NREUM.info={\"beacon\":\"bam.nr-data.net\",\"licenseKey\":\"9360826e15\",\"applicationID\":\"81062031\",\"transactionName\":\"MVNbZhQFXEAEVBBfWQgZeFESDV1dSkcFUVNLWFxFFQ==\",\"queueTime\":0,\"applicationTime\":500,\"atts\":\"HRRYEFwfT04=\",\"errorBeacon\":\"bam.nr-data.net\",\"agent\":\"\"}</script></body>\\n</html>\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need to convert this into a soup object\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup.find_all(name='h2',attrs={'class':'briefing-statement__title'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "urls = []\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'#main-content',\n",
       " u'https://www.whitehouse.gov',\n",
       " u'https://www.whitehouse.gov/news/?utm_source=link&utm_medium=header',\n",
       " u'https://www.whitehouse.gov/issues/economy-jobs/',\n",
       " u'https://www.whitehouse.gov/issues/national-security-defense/',\n",
       " u'https://www.whitehouse.gov/issues/budget-spending/',\n",
       " u'https://www.whitehouse.gov/issues/immigration/',\n",
       " u'https://www.whitehouse.gov/opioids/',\n",
       " u'https://www.whitehouse.gov/search/',\n",
       " u'https://www.whitehouse.gov/news/',\n",
       " u'https://www.whitehouse.gov/articles/',\n",
       " u'https://www.whitehouse.gov/presidential-actions/',\n",
       " u'https://www.whitehouse.gov/briefings-statements/',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=budget-spending',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=economy-jobs',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=education',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=energy-environment',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=foreign-policy',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=healthcare',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=immigration',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=infrastructure-technology',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=land-agriculture',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=law-justice',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=national-security-defense',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=social-programs',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=veterans',\n",
       " u'https://www.whitehouse.gov/briefings-statements/administrator-mcmahon-small-business-saturday-shows-big-economic-benefits-shopping-small/',\n",
       " u'https://www.whitehouse.gov/issues/economy-jobs/',\n",
       " u'https://www.whitehouse.gov/briefings-statements/remarks-president-trump-thanksgiving-teleconference-members-military/',\n",
       " u'https://www.whitehouse.gov/issues/national-security-defense/',\n",
       " u'https://www.whitehouse.gov/briefings-statements/remarks-president-trump-members-coast-guard/',\n",
       " u'https://www.whitehouse.gov/issues/national-security-defense/',\n",
       " u'https://www.whitehouse.gov/presidential-actions/president-donald-j-trump-announces-intent-nominate-individuals-key-administration-posts-3/',\n",
       " u'https://www.whitehouse.gov/briefings-statements/remarks-president-trump-national-thanksgiving-turkey-pardoning-ceremony-2/',\n",
       " u'https://www.whitehouse.gov/briefings-statements/president-donald-j-trump-signed-h-r-2615-law/',\n",
       " u'https://www.whitehouse.gov/issues/land-agriculture/',\n",
       " u'https://www.whitehouse.gov/briefings-statements/remarks-president-trump-marine-one-departure-25/',\n",
       " u'https://www.whitehouse.gov/briefings-statements/statement-press-secretary-41/',\n",
       " u'https://www.whitehouse.gov/issues/immigration/',\n",
       " u'https://www.whitehouse.gov/briefings-statements/president-donald-j-trump-urges-congressional-action-improve-forest-management-help-prevent-wildfires/',\n",
       " u'https://www.whitehouse.gov/issues/land-agriculture/',\n",
       " u'https://www.whitehouse.gov/briefings-statements/statement-president-donald-j-trump-standing-saudi-arabia/',\n",
       " u'https://www.whitehouse.gov/issues/foreign-policy/',\n",
       " u'https://www.whitehouse.gov/news/page/2/',\n",
       " u'https://www.whitehouse.gov/news/page/3/',\n",
       " u'https://www.whitehouse.gov/news/page/488/',\n",
       " u'https://www.whitehouse.gov/news/page/2/',\n",
       " u'https://www.whitehouse.gov',\n",
       " u'https://www.whitehouse.gov/live/',\n",
       " u'https://apply.whitehouse.gov/',\n",
       " u'https://www.whitehouse.gov/get-involved/',\n",
       " u'https://www.whitehouse.gov/copyright/',\n",
       " u'https://www.whitehouse.gov/privacy-policy/',\n",
       " u'https://twitter.com/whitehouse',\n",
       " u'https://www.facebook.com/WhiteHouse/',\n",
       " u'https://www.instagram.com/whitehouse/',\n",
       " u'/contact/',\n",
       " u'https://www.whitehouse.gov/news/',\n",
       " u'https://www.whitehouse.gov/briefings-statements/',\n",
       " u'https://www.whitehouse.gov/presidential-actions/',\n",
       " u'https://www.whitehouse.gov/articles/',\n",
       " u'https://www.whitehouse.gov/news/',\n",
       " u'https://www.whitehouse.gov/issues/',\n",
       " u'https://www.whitehouse.gov/issues/education/',\n",
       " u'https://www.whitehouse.gov/issues/national-security-defense/',\n",
       " u'https://www.whitehouse.gov/issues/budget-spending/',\n",
       " u'https://www.whitehouse.gov/issues/immigration/',\n",
       " u'https://www.whitehouse.gov/issues/healthcare/',\n",
       " u'https://www.whitehouse.gov/issues/',\n",
       " u'https://www.whitehouse.gov/the-trump-administration/',\n",
       " u'https://www.whitehouse.gov/people/donald-j-trump/',\n",
       " u'https://www.whitehouse.gov/people/mike-pence/',\n",
       " u'https://www.whitehouse.gov/people/melania-trump/',\n",
       " u'https://www.whitehouse.gov/people/karen-pence/',\n",
       " u'https://www.whitehouse.gov/the-trump-administration/the-cabinet/',\n",
       " u'https://www.whitehouse.gov/disclosures/',\n",
       " u'#',\n",
       " u'https://www.whitehouse.gov/cea/',\n",
       " u'https://www.whitehouse.gov/ceq/',\n",
       " u'https://www.whitehouse.gov/nsc/',\n",
       " u'https://www.whitehouse.gov/omb/',\n",
       " u'https://www.whitehouse.gov/ondcp/',\n",
       " u'https://www.whitehouse.gov/ostp/',\n",
       " u'https://www.whitehouse.gov/about-the-white-house/',\n",
       " u'https://www.whitehouse.gov/about-the-white-house/tours-events/',\n",
       " u'https://www.whitehouse.gov/about-the-white-house/presidents/',\n",
       " u'https://www.whitehouse.gov/about-the-white-house/first-ladies/',\n",
       " u'https://www.whitehouse.gov/about-the-white-house/the-grounds/',\n",
       " u'https://www.whitehouse.gov/about-the-white-house/our-government/',\n",
       " u'https://www.whitehouse.gov/get-involved/',\n",
       " u'https://www.whitehouse.gov/get-involved/write-or-call/',\n",
       " u'https://www.whitehouse.gov/get-involved/internships/',\n",
       " u'https://www.whitehouse.gov/get-involved/fellows/',\n",
       " u'https://www.whitehouse.gov/contact/',\n",
       " u'https://www.whitehouse.gov/live/']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "driver = webdriver.Chrome(\"C:\\Users\\dbell\\Data Science/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver.get(\"http://www.python.org\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: invalid session id\n  (Driver info: chromedriver=2.44.609538 (b655c5a60b0b544917107a59d4153d4bf78e1b90),platform=Windows NT 10.0.17134 x86_64)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mWebDriverException\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-bbbc7272b0ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# close it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\dbell\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.pyc\u001b[0m in \u001b[0;36mclose\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    686\u001b[0m             \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m         \"\"\"\n\u001b[1;32m--> 688\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCLOSE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mquit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\dbell\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.pyc\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32mC:\\Users\\dbell\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.pyc\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: invalid session id\n  (Driver info: chromedriver=2.44.609538 (b655c5a60b0b544917107a59d4153d4bf78e1b90),platform=Windows NT 10.0.17134 x86_64)\n"
     ]
    }
   ],
   "source": [
    "# close it\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#I did this very inelegantly.  Does not account for more than 488 pages.  Manually coded out rather than a loop.\n",
    "urls = []\n",
    "\n",
    "driver = webdriver.Chrome(\"C:\\Users\\dbell\\Data Science/chromedriver.exe\")\n",
    "driver.get(\"https://www.whitehouse.gov/news\")\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href']) \n",
    "driver.get('https://www.whitehouse.gov/news/page/2/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/3/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/4/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/5/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/6/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/7/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/8/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/9/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/10/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/11/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/12/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/13/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/14/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/15/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/16/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/17/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/18/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/19/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/20/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/21/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/22/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/23/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/24/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/25/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/26/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/27/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/28/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/29/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/30/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/31/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/32/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/33/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/34/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/35/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/36/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/37/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/38/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/39/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/40/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/41/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/42/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/43/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/44/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/45/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/46/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/47/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/48/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/49/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/50/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/51/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/52/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/53/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/54/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/55/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/56/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/57/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/58/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/59/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/60/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/61/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/62/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/63/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/64/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/65/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/66/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/67/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/68/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/69/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/70/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/71/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/72/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/73/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/74/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/75/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/76/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/77/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/78/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/79/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/80/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/81/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/82/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/83/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/84/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/85/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/86/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/87/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/88/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/89/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/90/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/91/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/92/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/93/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/94/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/95/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/96/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/97/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/98/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/99/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/100/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/101/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/102/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/103/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/104/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/105/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/106/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/107/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/108/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/109/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/110/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/111/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/112/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/113/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/114/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/115/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/116/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/117/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/118/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/119/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/120/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/121/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/122/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/123/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/124/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/125/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/126/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/127/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/128/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/129/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/130/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/131/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/132/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/133/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/134/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/135/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/136/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/137/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/138/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/139/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/140/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/141/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/142/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/143/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/144/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/145/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/146/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/147/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/148/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/149/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/150/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/151/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/152/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/153/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/154/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/155/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/156/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/157/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/158/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/159/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/160/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/161/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/162/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/163/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/164/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/165/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/166/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/167/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/168/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/169/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/170/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/171/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/172/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/173/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/174/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/175/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/176/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/177/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/178/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/179/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/180/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/181/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/182/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/183/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/184/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/185/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/186/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/187/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/188/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/189/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/190/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/191/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/192/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/193/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/194/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/195/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/196/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/197/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/198/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/199/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/200/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/201/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/202/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/203/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/204/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/205/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/206/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/207/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/208/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/209/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/210/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/211/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/212/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/213/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/214/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/215/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/216/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/217/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/218/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/219/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/220/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/221/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/222/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/223/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/224/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/225/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/226/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/227/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/228/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/229/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/230/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/231/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/232/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/233/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/234/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/235/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/236/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/237/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/238/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/239/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/240/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/241/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/242/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/243/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/244/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/245/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/246/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/247/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/248/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/249/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/250/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/251/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/252/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/253/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/254/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/255/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/256/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/257/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/258/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/259/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/260/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/261/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/262/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/263/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/264/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/265/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/266/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/267/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/268/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/269/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/270/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/271/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/272/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/273/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/274/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/275/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/276/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/277/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/278/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/279/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/280/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/281/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/282/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/283/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/284/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/285/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/286/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/287/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/288/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/289/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/290/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/291/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/292/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/293/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/294/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/295/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/296/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/297/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/298/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/299/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/300/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/301/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/302/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/303/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/304/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/305/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/306/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/307/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/308/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/309/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/310/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/311/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/312/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/313/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/314/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/315/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/316/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/317/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/318/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/319/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/320/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/321/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/322/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/323/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/324/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/325/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/326/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/327/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/328/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/329/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/330/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/331/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/332/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/333/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/334/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/335/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/336/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/337/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/338/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/339/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/340/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/341/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/342/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/343/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/344/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/345/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/346/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/347/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/348/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/349/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/350/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/351/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/352/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/353/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/354/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/355/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/356/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/357/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/358/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/359/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/360/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/361/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/362/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/363/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/364/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/365/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/366/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/367/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/368/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/369/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/370/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/371/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/372/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/373/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/374/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/375/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/376/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/377/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/378/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/379/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/380/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/381/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/382/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/383/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/384/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/385/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/386/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/387/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/388/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/389/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/390/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/391/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/392/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/393/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/394/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/395/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/396/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/397/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/398/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/399/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/400/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/401/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/402/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/403/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/404/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/405/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/406/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/407/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/408/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/409/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/410/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/411/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/412/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/413/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/414/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/415/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/416/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/417/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/418/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/419/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/420/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/421/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/422/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/423/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/424/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/425/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/426/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/427/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/428/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/429/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/430/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/431/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/432/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/433/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/434/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/435/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/436/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/437/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/438/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/439/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/440/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/441/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/442/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/443/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/444/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/445/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/446/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/447/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/448/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/449/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/450/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/451/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/452/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/453/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/454/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/455/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/456/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/457/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/458/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/459/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/460/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/461/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/462/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/463/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/464/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/465/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/466/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/467/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/468/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/469/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/470/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/471/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/472/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/473/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/474/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/475/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/476/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/477/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/478/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/479/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/480/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/481/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/482/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/483/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/484/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/485/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/486/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/487/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n",
    "driver.get('https://www.whitehouse.gov/news/page/488/')\n",
    "# wait one second\n",
    "sleep(1)\n",
    "#grab the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser', from_encoding=\"utf-8\")\n",
    "for a in soup.find_all('a', href=True):\n",
    "    urls.append(a['href'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://www.whitehouse.gov/news/page/2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'#main-content',\n",
       " u'https://www.whitehouse.gov',\n",
       " u'https://www.whitehouse.gov/news/?utm_source=link&utm_medium=header',\n",
       " u'https://www.whitehouse.gov/issues/economy-jobs/',\n",
       " u'https://www.whitehouse.gov/issues/national-security-defense/',\n",
       " u'https://www.whitehouse.gov/issues/budget-spending/',\n",
       " u'https://www.whitehouse.gov/issues/immigration/',\n",
       " u'https://www.whitehouse.gov/opioids/',\n",
       " u'https://www.whitehouse.gov/search/',\n",
       " u'https://www.whitehouse.gov/news/',\n",
       " u'https://www.whitehouse.gov/articles/',\n",
       " u'https://www.whitehouse.gov/presidential-actions/',\n",
       " u'https://www.whitehouse.gov/briefings-statements/',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=budget-spending',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=economy-jobs',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=education',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=energy-environment',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=foreign-policy',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=healthcare',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=immigration',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=infrastructure-technology',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=land-agriculture',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=law-justice',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=national-security-defense',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=social-programs',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=veterans',\n",
       " u'https://www.whitehouse.gov/briefings-statements/administrator-mcmahon-small-business-saturday-shows-big-economic-benefits-shopping-small/',\n",
       " u'https://www.whitehouse.gov/issues/economy-jobs/',\n",
       " u'https://www.whitehouse.gov/briefings-statements/remarks-president-trump-thanksgiving-teleconference-members-military/',\n",
       " u'https://www.whitehouse.gov/issues/national-security-defense/',\n",
       " u'https://www.whitehouse.gov/briefings-statements/remarks-president-trump-members-coast-guard/',\n",
       " u'https://www.whitehouse.gov/issues/national-security-defense/',\n",
       " u'https://www.whitehouse.gov/presidential-actions/president-donald-j-trump-announces-intent-nominate-individuals-key-administration-posts-3/',\n",
       " u'https://www.whitehouse.gov/briefings-statements/remarks-president-trump-national-thanksgiving-turkey-pardoning-ceremony-2/',\n",
       " u'https://www.whitehouse.gov/briefings-statements/president-donald-j-trump-signed-h-r-2615-law/',\n",
       " u'https://www.whitehouse.gov/issues/land-agriculture/',\n",
       " u'https://www.whitehouse.gov/briefings-statements/remarks-president-trump-marine-one-departure-25/',\n",
       " u'https://www.whitehouse.gov/briefings-statements/statement-press-secretary-41/',\n",
       " u'https://www.whitehouse.gov/issues/immigration/',\n",
       " u'https://www.whitehouse.gov/briefings-statements/president-donald-j-trump-urges-congressional-action-improve-forest-management-help-prevent-wildfires/',\n",
       " u'https://www.whitehouse.gov/issues/land-agriculture/',\n",
       " u'https://www.whitehouse.gov/briefings-statements/statement-president-donald-j-trump-standing-saudi-arabia/',\n",
       " u'https://www.whitehouse.gov/issues/foreign-policy/',\n",
       " u'https://www.whitehouse.gov/news/page/2/',\n",
       " u'https://www.whitehouse.gov/news/page/3/',\n",
       " u'https://www.whitehouse.gov/news/page/488/',\n",
       " u'https://www.whitehouse.gov/news/page/2/',\n",
       " u'https://www.whitehouse.gov',\n",
       " u'https://www.whitehouse.gov/live/',\n",
       " u'https://apply.whitehouse.gov/',\n",
       " u'https://www.whitehouse.gov/get-involved/',\n",
       " u'https://www.whitehouse.gov/copyright/',\n",
       " u'https://www.whitehouse.gov/privacy-policy/',\n",
       " u'https://twitter.com/whitehouse',\n",
       " u'https://www.facebook.com/WhiteHouse/',\n",
       " u'https://www.instagram.com/whitehouse/',\n",
       " u'/contact/',\n",
       " u'https://www.whitehouse.gov/news/',\n",
       " u'https://www.whitehouse.gov/briefings-statements/',\n",
       " u'https://www.whitehouse.gov/presidential-actions/',\n",
       " u'https://www.whitehouse.gov/articles/',\n",
       " u'https://www.whitehouse.gov/news/',\n",
       " u'https://www.whitehouse.gov/issues/',\n",
       " u'https://www.whitehouse.gov/issues/education/',\n",
       " u'https://www.whitehouse.gov/issues/national-security-defense/',\n",
       " u'https://www.whitehouse.gov/issues/budget-spending/',\n",
       " u'https://www.whitehouse.gov/issues/immigration/',\n",
       " u'https://www.whitehouse.gov/issues/healthcare/',\n",
       " u'https://www.whitehouse.gov/issues/',\n",
       " u'https://www.whitehouse.gov/the-trump-administration/',\n",
       " u'https://www.whitehouse.gov/people/donald-j-trump/',\n",
       " u'https://www.whitehouse.gov/people/mike-pence/',\n",
       " u'https://www.whitehouse.gov/people/melania-trump/',\n",
       " u'https://www.whitehouse.gov/people/karen-pence/',\n",
       " u'https://www.whitehouse.gov/the-trump-administration/the-cabinet/',\n",
       " u'https://www.whitehouse.gov/disclosures/',\n",
       " u'#',\n",
       " u'https://www.whitehouse.gov/cea/',\n",
       " u'https://www.whitehouse.gov/ceq/',\n",
       " u'https://www.whitehouse.gov/nsc/',\n",
       " u'https://www.whitehouse.gov/omb/',\n",
       " u'https://www.whitehouse.gov/ondcp/',\n",
       " u'https://www.whitehouse.gov/ostp/',\n",
       " u'https://www.whitehouse.gov/about-the-white-house/',\n",
       " u'https://www.whitehouse.gov/about-the-white-house/tours-events/',\n",
       " u'https://www.whitehouse.gov/about-the-white-house/presidents/',\n",
       " u'https://www.whitehouse.gov/about-the-white-house/first-ladies/',\n",
       " u'https://www.whitehouse.gov/about-the-white-house/the-grounds/',\n",
       " u'https://www.whitehouse.gov/about-the-white-house/our-government/',\n",
       " u'https://www.whitehouse.gov/get-involved/',\n",
       " u'https://www.whitehouse.gov/get-involved/write-or-call/',\n",
       " u'https://www.whitehouse.gov/get-involved/internships/',\n",
       " u'https://www.whitehouse.gov/get-involved/fellows/',\n",
       " u'https://www.whitehouse.gov/contact/',\n",
       " u'https://www.whitehouse.gov/live/',\n",
       " u'#main-content',\n",
       " u'https://www.whitehouse.gov',\n",
       " u'https://www.whitehouse.gov/news/?utm_source=link&utm_medium=header',\n",
       " u'https://www.whitehouse.gov/issues/economy-jobs/',\n",
       " u'https://www.whitehouse.gov/issues/national-security-defense/',\n",
       " u'https://www.whitehouse.gov/issues/budget-spending/',\n",
       " u'https://www.whitehouse.gov/issues/immigration/',\n",
       " u'https://www.whitehouse.gov/opioids/',\n",
       " u'https://www.whitehouse.gov/search/',\n",
       " u'https://www.whitehouse.gov/news/',\n",
       " u'https://www.whitehouse.gov/articles/',\n",
       " u'https://www.whitehouse.gov/presidential-actions/',\n",
       " u'https://www.whitehouse.gov/briefings-statements/',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=budget-spending',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=economy-jobs',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=education',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=energy-environment',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=foreign-policy',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=healthcare',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=immigration',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=infrastructure-technology',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=land-agriculture',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=law-justice',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=national-security-defense',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=social-programs',\n",
       " u'https://www.whitehouse.gov/news/?issue_filter=veterans',\n",
       " u'https://www.whitehouse.gov/presidential-actions/presidential-proclamation-thanksgiving-day-2018/',\n",
       " u'https://www.whitehouse.gov/briefings-statements/american-economy-rises-new-heights/',\n",
       " u'https://www.whitehouse.gov/issues/economy-jobs/',\n",
       " u'https://www.whitehouse.gov/briefings-statements/kushner-philipson-criminal-justice-reform-can-improve-expensive-ineffective-system-lowering-recidivism/',\n",
       " u'https://www.whitehouse.gov/issues/law-justice/',\n",
       " u'https://www.whitehouse.gov/briefings-statements/president-trumps-administration-advancing-free-open-indo-pacific-investments-partnerships-economics-security-governance/',\n",
       " u'https://www.whitehouse.gov/issues/foreign-policy/',\n",
       " u'https://www.whitehouse.gov/briefings-statements/remarks-vice-president-pence-announcement-papua-new-guinea-electrification-partnership/',\n",
       " u'https://www.whitehouse.gov/issues/foreign-policy/',\n",
       " u'https://www.whitehouse.gov/briefings-statements/remarks-president-trump-tour-malibu-neighborhood/',\n",
       " u'https://www.whitehouse.gov/issues/land-agriculture/',\n",
       " u'https://www.whitehouse.gov/briefings-statements/remarks-vice-president-pence-thanksgiving-gathering-military-families/',\n",
       " u'https://www.whitehouse.gov/briefings-statements/remarks-president-trump-incident-command-post-briefing-chico-california/',\n",
       " u'https://www.whitehouse.gov/issues/land-agriculture/',\n",
       " u'https://www.whitehouse.gov/briefings-statements/papua-new-guinea-electrification-partnership/',\n",
       " u'https://www.whitehouse.gov/issues/foreign-policy/',\n",
       " u'https://www.whitehouse.gov/briefings-statements/remarks-president-trump-marine-one-departure-24/',\n",
       " u'https://www.whitehouse.gov/news/',\n",
       " u'https://www.whitehouse.gov/news/',\n",
       " u'https://www.whitehouse.gov/news/page/3/',\n",
       " u'https://www.whitehouse.gov/news/page/4/',\n",
       " u'https://www.whitehouse.gov/news/page/488/',\n",
       " u'https://www.whitehouse.gov/news/page/3/',\n",
       " u'https://www.whitehouse.gov',\n",
       " u'https://www.whitehouse.gov/live/',\n",
       " u'https://apply.whitehouse.gov/',\n",
       " u'https://www.whitehouse.gov/get-involved/',\n",
       " u'https://www.whitehouse.gov/copyright/',\n",
       " u'https://www.whitehouse.gov/privacy-policy/',\n",
       " u'https://twitter.com/whitehouse',\n",
       " u'https://www.facebook.com/WhiteHouse/',\n",
       " u'https://www.instagram.com/whitehouse/',\n",
       " u'/contact/',\n",
       " u'https://www.whitehouse.gov/news/',\n",
       " u'https://www.whitehouse.gov/briefings-statements/',\n",
       " u'https://www.whitehouse.gov/presidential-actions/',\n",
       " u'https://www.whitehouse.gov/articles/',\n",
       " u'https://www.whitehouse.gov/news/',\n",
       " u'https://www.whitehouse.gov/issues/',\n",
       " u'https://www.whitehouse.gov/issues/education/',\n",
       " u'https://www.whitehouse.gov/issues/national-security-defense/',\n",
       " u'https://www.whitehouse.gov/issues/budget-spending/',\n",
       " u'https://www.whitehouse.gov/issues/immigration/',\n",
       " u'https://www.whitehouse.gov/issues/healthcare/',\n",
       " u'https://www.whitehouse.gov/issues/',\n",
       " u'https://www.whitehouse.gov/the-trump-administration/',\n",
       " u'https://www.whitehouse.gov/people/donald-j-trump/',\n",
       " u'https://www.whitehouse.gov/people/mike-pence/',\n",
       " u'https://www.whitehouse.gov/people/melania-trump/',\n",
       " u'https://www.whitehouse.gov/people/karen-pence/',\n",
       " u'https://www.whitehouse.gov/the-trump-administration/the-cabinet/',\n",
       " u'https://www.whitehouse.gov/disclosures/',\n",
       " u'#',\n",
       " u'https://www.whitehouse.gov/cea/',\n",
       " u'https://www.whitehouse.gov/ceq/',\n",
       " u'https://www.whitehouse.gov/nsc/',\n",
       " u'https://www.whitehouse.gov/omb/',\n",
       " u'https://www.whitehouse.gov/ondcp/',\n",
       " u'https://www.whitehouse.gov/ostp/',\n",
       " u'https://www.whitehouse.gov/about-the-white-house/',\n",
       " u'https://www.whitehouse.gov/about-the-white-house/tours-events/',\n",
       " u'https://www.whitehouse.gov/about-the-white-house/presidents/',\n",
       " u'https://www.whitehouse.gov/about-the-white-house/first-ladies/',\n",
       " u'https://www.whitehouse.gov/about-the-white-house/the-grounds/',\n",
       " u'https://www.whitehouse.gov/about-the-white-house/our-government/',\n",
       " u'https://www.whitehouse.gov/get-involved/',\n",
       " u'https://www.whitehouse.gov/get-involved/write-or-call/',\n",
       " u'https://www.whitehouse.gov/get-involved/internships/',\n",
       " u'https://www.whitehouse.gov/get-involved/fellows/',\n",
       " u'https://www.whitehouse.gov/contact/',\n",
       " u'https://www.whitehouse.gov/live/']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-1a0712bafabd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0murls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'white_house_urls.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "urls.to_csv('white_house_urls.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('white_house_urls.csv', 'wb') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'thefile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-b57121e62206>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'white_house_urls.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0murls\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[1;32mprint\u001b[0m \u001b[1;33m>>\u001b[0m \u001b[0mthefile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'thefile' is not defined"
     ]
    }
   ],
   "source": [
    "with open('white_house_urls.txt', 'w') as f:\n",
    "    for item in urls:\n",
    "        print >> thefile, item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('white_house_urls.txt', 'w') as f:\n",
    "    for item in urls:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now to fetch content from these urls!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# now print out EACH location for the restaurants\n",
    "for website in soup.find_all(name='h2',attrs={'class':'briefing-statement__title'}):\n",
    "    print website.href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_html = '''<a href=\"some_url\">next</a>\n",
    "<span class=\"class\"><a href=\"another_url\">later</a></span>'''\n",
    "\n",
    "soup = BeautifulSoup(url_html)\n",
    "\n",
    "for a in soup.find_all('a', href=True):\n",
    "    print \"Found the URL:\", a['href']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
