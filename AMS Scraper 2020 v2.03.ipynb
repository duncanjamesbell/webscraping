{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib \n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import urllib.request\n",
    "import webbrowser\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import string\n",
    "import numpy as np\n",
    "from bs4.element import Tag\n",
    "from bs4.dammit import EncodingDetector\n",
    "\n",
    "internal_only = True #parameter which determines whether we will restrict software scraping to internal URLs, ie they have the parent domain\n",
    "scrape_login_links = True #set to true to scrape URLs found on login pages also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import shutil\n",
    "'''\n",
    "with io.open('cupola urls.csv', encoding='utf-8', errors='ignore') as source:\n",
    "    with io.open('cupola urls_utf.csv', mode='w', encoding='utf-8') as target:\n",
    "        shutil.copyfileobj(source,target)\n",
    "\n",
    "df = pd.read_csv('cupola urls_utf.csv')\n",
    "'''\n",
    "\n",
    "with io.open('Tech survey test4.csv', encoding='utf-8', errors='ignore') as source:\n",
    "    with io.open('Tech survey test4_utf.csv', mode='w', encoding='utf-8') as target:\n",
    "        shutil.copyfileobj(source,target)\n",
    "\n",
    "df = pd.read_csv('Tech survey test4_utf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>organization_id</th>\n",
       "      <th>name</th>\n",
       "      <th>url</th>\n",
       "      <th>batch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ATSSA</td>\n",
       "      <td>https://www.atssa.com/</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>The national Council</td>\n",
       "      <td>https://www.thenationalcouncil.org/</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  organization_id                  name  \\\n",
       "0           0                1                 ATSSA   \n",
       "1           1                8  The national Council   \n",
       "\n",
       "                                   url  batch  \n",
       "0               https://www.atssa.com/      1  \n",
       "1  https://www.thenationalcouncil.org/      2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionaries and suppression lists\n",
    "exclude_keywords= [\n",
    "    'twitter',\n",
    "    'facebook',\n",
    "    'youtube',\n",
    "    'youtu.be',\n",
    "    'pinterest',\n",
    "    'instagram',\n",
    "    'linkedin',\n",
    "    'paypal',\n",
    "    'venmo',\n",
    "    'javascript',\n",
    "    '@', \n",
    "    'google',\n",
    "    'flickr'\n",
    "]\n",
    "\n",
    "#note exceptions to keyword rules here\n",
    "exceptions_dict = {\n",
    "    'iMIS':['optimist','maximise','optimism','optimised'],\n",
    "    'Amilia':['familiar','unfamiliar','familial','familiarity'],\n",
    "    'Tendenci':['tendencies'],\n",
    "    'The Associate':['the associated'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import shutil\n",
    "\n",
    "with io.open('software lkup v1.02.csv', encoding='utf-8', errors='ignore') as source:\n",
    "    with io.open('software list_utf.csv', mode='w', encoding='utf-8') as target:\n",
    "        shutil.copyfileobj(source,target)\n",
    "\n",
    "software = pd.read_csv('software list_utf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>software</th>\n",
       "      <th>software2</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>service_category_id</th>\n",
       "      <th>service_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123 Signup</td>\n",
       "      <td>123Signup</td>\n",
       "      <td>235614.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>AMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123Signup</td>\n",
       "      <td>123Signup</td>\n",
       "      <td>235614.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>AMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACGI PAGE</td>\n",
       "      <td>Association Anywhere</td>\n",
       "      <td>235619.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>AMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AdvantageCS</td>\n",
       "      <td>AdvantageCS</td>\n",
       "      <td>235589.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>AMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alliance by Protech</td>\n",
       "      <td>Alliance</td>\n",
       "      <td>235590.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>AMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>WildApricot</td>\n",
       "      <td>Wild Apricot</td>\n",
       "      <td>235611.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>AMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>xCatalyst</td>\n",
       "      <td>xCatalyst</td>\n",
       "      <td>235674.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>AMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Your People</td>\n",
       "      <td>Your People</td>\n",
       "      <td>235675.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>AMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>YourMembership</td>\n",
       "      <td>YourMembership</td>\n",
       "      <td>235612.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>AMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>YourPeople</td>\n",
       "      <td>Your People</td>\n",
       "      <td>235675.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>AMS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                software             software2  vendor_id  \\\n",
       "0             123 Signup             123Signup   235614.0   \n",
       "1              123Signup             123Signup   235614.0   \n",
       "2              ACGI PAGE  Association Anywhere   235619.0   \n",
       "3            AdvantageCS           AdvantageCS   235589.0   \n",
       "4    Alliance by Protech              Alliance   235590.0   \n",
       "..                   ...                   ...        ...   \n",
       "109          WildApricot          Wild Apricot   235611.0   \n",
       "110            xCatalyst             xCatalyst   235674.0   \n",
       "111          Your People           Your People   235675.0   \n",
       "112       YourMembership        YourMembership   235612.0   \n",
       "113           YourPeople           Your People   235675.0   \n",
       "\n",
       "     service_category_id service_category  \n",
       "0                   12.0              AMS  \n",
       "1                   12.0              AMS  \n",
       "2                   12.0              AMS  \n",
       "3                   12.0              AMS  \n",
       "4                   12.0              AMS  \n",
       "..                   ...              ...  \n",
       "109                 12.0              AMS  \n",
       "110                 12.0              AMS  \n",
       "111                 12.0              AMS  \n",
       "112                 12.0              AMS  \n",
       "113                 12.0              AMS  \n",
       "\n",
       "[114 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'123 Signup': '123Signup',\n",
       " '123Signup': '123Signup',\n",
       " 'ACGI PAGE': 'Association Anywhere',\n",
       " 'AdvantageCS': 'AdvantageCS',\n",
       " 'Alliance by Protech': 'Alliance',\n",
       " 'Altai': 'Altai',\n",
       " 'Amilia': 'Amilia',\n",
       " 'Amilia.com': 'Amilia',\n",
       " 'AMPAC': 'AMPAC',\n",
       " 'Aptify': 'Aptify',\n",
       " 'Association Online': 'Association Online ',\n",
       " 'BenchPrep': 'BenchPrep',\n",
       " 'Bitrix 24': 'Bitrix24',\n",
       " 'Bitrix24': 'Bitrix24',\n",
       " 'Blue Sky eLearn': 'Blue Sky eLearn',\n",
       " 'BlueSkyeLearn': 'Blue Sky eLearn',\n",
       " 'Brightspace': 'Brightspace',\n",
       " 'Broad Point Engage': 'BroadPoint Engage',\n",
       " 'BroadPoint Engage': 'BroadPoint Engage',\n",
       " 'BroadPointEngage': 'BroadPoint Engage',\n",
       " 'Cadmium': 'CadmiumCD',\n",
       " 'CadmiumCD': 'CadmiumCD',\n",
       " 'Classroom 24-7': 'Classroom 24/7',\n",
       " 'Classroom24-7': 'Classroom 24/7',\n",
       " 'ClearVantage': 'ClearVantage',\n",
       " 'ClubExpress': 'ClubExpress',\n",
       " 'cobalt.saas': 'Cobalt',\n",
       " 'CourseStage': 'CourseStage',\n",
       " 'Crowd Wisdom': 'Crowd Wisdom',\n",
       " 'CrowdWisdom': 'Crowd Wisdom',\n",
       " 'Cvent': 'Cvent',\n",
       " 'Elevate LMS': 'Elevate LMS',\n",
       " 'ElevateLMS': 'Elevate LMS',\n",
       " 'Engage Core': 'Engage AMS',\n",
       " 'EngageCore': 'Engage AMS',\n",
       " 'Engagifii': 'Engagifii',\n",
       " 'Ethos CE': 'EthosCE',\n",
       " 'EthosCE': 'EthosCE',\n",
       " 'Event Bank': 'EventBank',\n",
       " 'EventBank': 'EventBank',\n",
       " 'Event Mobi': 'EventMobi',\n",
       " 'EventMobi': 'EventMobi',\n",
       " 'Exware': 'Exware',\n",
       " 'Fonteva': 'Fonteva',\n",
       " 'Freestone by Community Brands': 'Freestone',\n",
       " 'Freestone LMS': 'Freestone',\n",
       " 'FreestoneLMS': 'Freestone',\n",
       " 'GoMembers': 'GoMembers',\n",
       " 'GoMembership': 'GoMembership',\n",
       " 'GroupAhead': 'GroupAhead',\n",
       " 'GrowthZone': 'GrowthZone',\n",
       " 'Guild': 'Guild',\n",
       " 'Higher Logic': 'Higher Logic',\n",
       " 'HigherLogic': 'Higher Logic',\n",
       " 'iCohere': 'iCohere',\n",
       " 'iMIS-WebPart': 'iMIS',\n",
       " 'Impexium': 'Impexium',\n",
       " 'Internet4Associations': 'I4A',\n",
       " 'KnowledgeDirectWeb': 'Knowledge Direct',\n",
       " 'MatrixMaxx': 'MatrixMaxx',\n",
       " 'mElimu': 'mElimu',\n",
       " 'Member365': 'Member365',\n",
       " 'Membercast': 'Membercast',\n",
       " 'Memberclicks': 'Memberclicks ',\n",
       " 'MemberConnection': 'MemberConnection',\n",
       " 'MemberLeap': 'MemberLeap',\n",
       " 'MemberMax': 'MemberMax',\n",
       " 'MemberNova': 'MemberNova',\n",
       " 'MemberPlanet': 'MemberPlanet',\n",
       " 'MemberPlex': 'MemberPlex',\n",
       " 'MembershipWorks': 'MembershipWorks',\n",
       " 'MemberSuite': 'MemberSuite',\n",
       " 'Memex': 'Memex',\n",
       " 'Meridian LMS': 'Meridian LMS',\n",
       " 'MeridianLMS': 'Meridian LMS',\n",
       " 'Naylor': 'Naylor',\n",
       " 'Naylor Association Solutions': 'Naylor',\n",
       " 'NaylorNetwork': 'Naylor',\n",
       " 'Neon CRM': 'Neon CRM',\n",
       " 'NeonCRM': 'Neon CRM',\n",
       " 'Netforum': 'Netforum',\n",
       " 'NetFORUM Enterprise': 'NetFORUM Enterprise',\n",
       " 'NetFORUM Pro': 'NetFORUM Pro',\n",
       " 'NextThought': 'NextThought',\n",
       " 'Nimble AMS': 'NimbleAMS',\n",
       " 'NimbleAMS': 'NimbleAMS',\n",
       " 'NOAH_Users': 'NOAH',\n",
       " 'NOAHSignInUser': 'NOAH',\n",
       " 'noviAMS': 'noviAMS',\n",
       " 'Oasis LMS': 'Oasis LMS',\n",
       " 'Personify': 'Personify360',\n",
       " 'Personify360': 'Personify360',\n",
       " 'PersonifyEBusiness': 'Personify360',\n",
       " 'Powered by Tendenci': 'Tendenci',\n",
       " 'Protech': 'Alliance',\n",
       " 'Protech Associates': 'Alliance',\n",
       " 'ProtechAssociates': 'Alliance',\n",
       " 'Raklet': 'Raklet',\n",
       " 'SilkStart': 'SilkStart',\n",
       " 'StarChapter': 'StarChapter',\n",
       " 'Tendenci.com': 'Tendenci',\n",
       " 'The Associate': 'The Associate',\n",
       " 'TidyHQ': 'TidyHQ',\n",
       " 'Tovuti': 'Tovuti',\n",
       " 'WBT Systems': 'WBT Systems',\n",
       " 'WBTSystems': 'WBT Systems',\n",
       " 'Weblink': 'Weblink',\n",
       " 'Wicket': 'Wicket',\n",
       " 'Wild Apricot': 'Wild Apricot',\n",
       " 'WildApricot': 'Wild Apricot',\n",
       " 'xCatalyst': 'xCatalyst',\n",
       " 'Your People': 'Your People',\n",
       " 'YourMembership': 'YourMembership',\n",
       " 'YourPeople': 'Your People'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = pd.Series(software.software2.values,index=software.software).to_dict()\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = list(df.batch.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change this number to correspond with the number of batches you are running. \n",
    "list = range(0,301)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tlinks scraped from main URLs --- 16.39 seconds ---\n",
      "\tlinks scraped from login URLs --- 12.03 seconds ---\n",
      "\tsoftware scraped --- 318.75 seconds ---\n",
      "Batch time elapsed --- 347.1958327293396 seconds ---\n",
      "Starting batch 2\n",
      "\tlinks scraped from main URLs --- 31.02 seconds ---\n",
      "\tlinks scraped from login URLs --- 45.22 seconds ---\n",
      "\tsoftware scraped --- 401.41 seconds ---\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'software_2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-66190bd26d28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[0msoftware\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'internal_link'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minternal\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m     \u001b[0msoftware\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Batch time elapsed --- %s seconds ---\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Overall process time elapsed --- %s seconds ---\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0moriginal_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   3226\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3227\u001b[0m         )\n\u001b[1;32m-> 3228\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3230\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    181\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m                 \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m             )\n\u001b[0;32m    185\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    397\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    400\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m             \u001b[1;31m# No explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'software_2.csv'"
     ]
    }
   ],
   "source": [
    "original_time = time.time()\n",
    "batch_files = []\n",
    "\n",
    "#column where we'll store the total links found per org\n",
    "df['total_links'] = ''\n",
    "\n",
    "for batch in batches:\n",
    "    print('Starting batch ' + str(batch))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    step_time = time.time()\n",
    "    filename = 'software_' + str(batch) + '.csv'\n",
    "    batch_files.append(filename)\n",
    "\n",
    "    batch_df = df[df.batch == batch]\n",
    "    batch_df['url'] = batch_df['url'].str.lower()\n",
    "\n",
    "    #clean urls to standard format\n",
    "    clean_urls = []\n",
    "    for url in batch_df['url']: \n",
    "        if 'http://www.' in url:\n",
    "            clean_urls.append(url)\n",
    "        elif 'https://www.' in url:\n",
    "            clean_urls.append(url)\n",
    "        elif ('www.' in url) & ('http://' not in url) & ('https://' not in url):\n",
    "            clean_urls.append('http://' + url )\n",
    "        else: \n",
    "            clean_urls.append('http://www.' + url)\n",
    "    batch_df['clean_urls'] = clean_urls\n",
    "    \n",
    "    #derive domain root from clean url\n",
    "    domain_find = []\n",
    "    domain = []\n",
    "    for clean_url in batch_df['clean_urls']:\n",
    "        if 'https://' in url:\n",
    "            domain_find.append(clean_url[12:])\n",
    "        else:\n",
    "            domain_find.append(clean_url[11:])\n",
    "\n",
    "    for item in domain_find:\n",
    "        root = item.find('.')\n",
    "        r = item[:root]\n",
    "        domain.append(r)\n",
    "\n",
    "    batch_df['domain'] = domain\n",
    "    \n",
    "    domain_dict = pd.Series(batch_df.domain.values,index=batch_df.organization_id).to_dict()\n",
    "    \n",
    "    #this prevents files from downloading while the scraper visits sites, \n",
    "    #make sure chromedriver is compatible with your chrome version\n",
    "    \n",
    "    options = webdriver.ChromeOptions()\n",
    "    \n",
    "    prefs = {\n",
    "    'download_restrictions': 3,\n",
    "    }\n",
    "    options.add_experimental_option(\n",
    "    'prefs', prefs\n",
    "    )\n",
    "    \n",
    "    \n",
    "    #compile links to scrape\n",
    "    driver = webdriver.Chrome('C:/Users/dbell/Data Science [Github]/chromedriver.exe', options = options)\n",
    "\n",
    "    links = []\n",
    "    o_id = []\n",
    "    for index, row in batch_df.iterrows():\n",
    "        try:\n",
    "        \n",
    "            driver.get(row['clean_urls'])\n",
    "            o_id.append(row['organization_id'])\n",
    "            links.append(row['clean_urls'])\n",
    "\n",
    "            for a in driver.find_elements_by_xpath('.//a'):\n",
    "                try:\n",
    "                    link = a.get_attribute('href')\n",
    "                    if link is not None:\n",
    "                        #excluding bad rows\n",
    "                        if (isinstance(link, float) is False) & (len(str(link))>5):\n",
    "                            e=0\n",
    "                            for w in exclude_keywords:\n",
    "                                if w in link:\n",
    "                                    e=e+1\n",
    "                            if e==0: #there are no bad keywords present\n",
    "                                links.append(link)\n",
    "                                o_id.append(row['organization_id'])\n",
    "                except:\n",
    "                    a\n",
    "        except:\n",
    "            index\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    links_df = pd.DataFrame()\n",
    "    links_df['organization_id'] = o_id\n",
    "    links_df['link'] = links\n",
    "    links_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    if scrape_login_links == False:\n",
    "        all_links = links_df.copy()\n",
    "    \n",
    "    print(\"\\tlinks scraped from main URLs --- %s seconds ---\" % round(time.time() - step_time, 2))\n",
    "    \n",
    "    if scrape_login_links == True:\n",
    "        step_time = time.time()\n",
    "\n",
    "        #appending any links present on login pages, as these have a higher likelihood for having software tags\n",
    "\n",
    "        driver = webdriver.Chrome('C:/Users/dbell/Data Science [Github]/chromedriver.exe', options = options)\n",
    "\n",
    "        login_links = []\n",
    "        login_o_id = []\n",
    "        #iterate through all collected websites so far to see if they contain the word \"password\"; collect URLs if so\n",
    "        for index, row in links_df.iterrows():\n",
    "            if (internal_only == False) | ((internal_only == True) & (domain_dict[row['organization_id']] not in str(row['link']).lower())):\n",
    "                try:\n",
    "                    driver.get(row['link'])\n",
    "                    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                    string_soup = str(soup).lower()\n",
    "\n",
    "                    if 'password' in string_soup:\n",
    "                        for a in driver.find_elements_by_xpath('.//a'):\n",
    "                            try:\n",
    "                                link = a.get_attribute('href')\n",
    "                                if link is not None:\n",
    "                                    #excluding bad rows\n",
    "                                    if (isinstance(link, float) is False) & (len(str(link))>5):\n",
    "                                        e=0\n",
    "                                        for w in exclude_keywords:\n",
    "                                            if w in link:\n",
    "                                                e=e+1\n",
    "                                        if e==0: #there are no bad keywords present\n",
    "                                            login_links.append(link)\n",
    "                                            login_o_id.append(row['organization_id'])\n",
    "                            except:\n",
    "                                a\n",
    "                except:\n",
    "                    index\n",
    "\n",
    "        driver.close()\n",
    "\n",
    "        login_links_df = pd.DataFrame()\n",
    "        login_links_df['organization_id'] = login_o_id\n",
    "        login_links_df['link'] = login_links\n",
    "        #merge the original set of links plus any login page URLs together\n",
    "        all_links = pd.concat([links_df,login_links_df])\n",
    "        all_links.drop_duplicates(inplace=True)\n",
    "\n",
    "        print(\"\\tlinks scraped from login URLs --- %s seconds ---\" % round(time.time() - step_time, 2))\n",
    "    \n",
    "    step_time = time.time()\n",
    "    \n",
    "    #fill in the total_links column in df\n",
    "    for index, row in df.iterrows():\n",
    "        if row['organization_id'] in batch_df.organization_id.values:\n",
    "            try: \n",
    "                df.at[index,'total_links'] = all_links[all_links.organization_id == row['organization_id']].shape[0]\n",
    "            except:\n",
    "                df.at[index,'total_links'] = 0\n",
    "    \n",
    "    #here we are identifying all org \"internal\" links collected\n",
    "    internal_links = []\n",
    "    for index, row in all_links.iterrows():\n",
    "        if domain_dict[row['organization_id']] in str(row['link']).lower():\n",
    "            internal_links.append(1)\n",
    "        else:\n",
    "            internal_links.append(0)\n",
    "\n",
    "    all_links['internal_link'] = internal_links\n",
    "    #all_links.to_csv('all links duncan test.csv')\n",
    "    \n",
    "    #iterate through each link to scrape software references\n",
    "    org_ids = []\n",
    "    ams_tags = []\n",
    "    link = []\n",
    "    internal = []\n",
    "\n",
    "    #determine the scope based on internal_only parameter\n",
    "    if internal_only:\n",
    "        scrape_links = all_links[all_links.internal_link == 1]\n",
    "    else:\n",
    "        scrape_links = all_links.copy()\n",
    "    \n",
    "    driver = webdriver.Chrome('C:/Users/dbell/Data Science [Github]/chromedriver.exe', options = options)\n",
    "\n",
    "    #we're now ready to search for software keywords in the HTML of our universe of URLs, these are stored in a dataframe\n",
    "    for index, row in scrape_links.iterrows(): \n",
    "        try:\n",
    "            driver.get(row['link'])\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            string_soup = str(soup).lower()\n",
    "            for k in d:\n",
    "                if str(k).lower() in string_soup:\n",
    "                    if k in exceptions_dict:\n",
    "                        include = True\n",
    "                        #need to loop through the list of exception words here\n",
    "                        for e in exceptions_dict[k]:\n",
    "                            if e in string_soup:\n",
    "                                include = False\n",
    "                        if include:\n",
    "                            org_ids.append(row['organization_id'])\n",
    "                            link.append(row['link'])\n",
    "                            ams_tags.append(d[k])\n",
    "                            internal.append(row['internal_link'])\n",
    "                    else:\n",
    "                        org_ids.append(row['organization_id'])\n",
    "                        link.append(row['link'])\n",
    "                        ams_tags.append(d[k]) \n",
    "                        internal.append(row['internal_link'])\n",
    "\n",
    "        except:\n",
    "            index\n",
    "\n",
    "    driver.close()\n",
    "    \n",
    "    print(\"\\tsoftware scraped --- %s seconds ---\" % round(time.time() - step_time, 2))\n",
    "\n",
    "    software = pd.DataFrame()\n",
    "    #software['batch'] = batch\n",
    "    software['organization_id'] = org_ids\n",
    "    software['source_link'] = link\n",
    "    software['software'] = ams_tags\n",
    "    software['internal_link'] = internal\n",
    "    \n",
    "    software.to_csv(filename, index=False)\n",
    "    print(\"Batch time elapsed --- %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"Overall process time elapsed --- %s seconds ---\" % (time.time() - original_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptions_dict['iMIS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps/ Development Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I want to concat all of the software outputs and do the following:\n",
    "    * dedup on org id and software\n",
    "    * do some type of formula to predict the \"confidence\" of the software prediction by (idea based on spot check finds):\n",
    "        * creating a point system for key type (keyword, code line)\n",
    "        * count the amount of unique pages each software is found for an org id \n",
    "        * and maybe count how many times the string that key in contained in was found in ALL urls\n",
    "        \n",
    "    * Write code that has output in link_org_vendor form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This puts all output files together, needs to improve bcuz it pulls all column headers\n",
    "\n",
    "all_software = pd.DataFrame(columns=['organization_id','source_link','software', 'internal_link'])\n",
    "for file in batch_files:\n",
    "    file_df = pd.read_csv(file)\n",
    "    all_software = pd.concat([all_software,file_df])\n",
    "    all_software.reset_index(inplace=True)\n",
    "    try:\n",
    "        all_software.drop(columns=['index'],inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        all_software.drop(columns=['level_0'],inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "\n",
    "'''\n",
    "#old way\n",
    "#This is used so that range is customized based on dataframe\n",
    "min = df.batch.min()\n",
    "max = df.batch.max()\n",
    "\n",
    "fout = open ('out.csv','a')\n",
    "# first file:\n",
    "for line in open('software_1.csv'):\n",
    "    fout.write(line)\n",
    "# now the rest:    \n",
    "for num in range((min + 1), (max + 1)):\n",
    "    f = open('software_'+str(num)+'.csv')\n",
    "    next(f) # skip the header\n",
    "    for line in f:\n",
    "         fout.write(line)\n",
    "    f.close() # not really needed\n",
    "fout.close()\n",
    "\n",
    "all_software = pd.read_csv('out.csv')\n",
    "#all_software.drop_duplicates(inplace=True)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_software.loc[39]['source_link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will use this to see how many times a software is found related to each org\n",
    "all_software_grouped = all_software.drop(columns=['internal_link'])\n",
    "all_software_grouped= all_software_grouped.groupby(['organization_id', 'software']).count()\n",
    "all_software_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_software_grouped.reset_index(inplace=True)\n",
    "merged = pd.merge(df,all_software_grouped,how='left',left_on='organization_id',right_on='organization_id')\n",
    "\n",
    "#some code we can use to create and maintain a 'master' table of all software assigned by the scraper\n",
    "#should add date software discovered\n",
    "try:\n",
    "    old_master = pd.read_csv('master.csv')\n",
    "    master = old_master.concat([old_master,merged])\n",
    "    master = master.drop_duplicates()\n",
    "except:\n",
    "    merged.to_csv('master.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
